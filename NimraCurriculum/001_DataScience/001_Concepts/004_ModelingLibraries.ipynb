{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Modeling Libraries in Python\n",
    "\n",
    "In this book, I have focused on providing a programming foundation for doing data analysis in Python. Since data analysts and scientists often report spending a disproportionate amount of time with data wrangling and preparation, the book’s structure reflects the importance of mastering these techniques.\n",
    "\n",
    "Which library you use for developing models will depend on the application. Many statistical problems can be solved by simpler techniques like ordinary least squares regression, while other problems may call for more advanced machine learning methods. Fortunately, Python has become one of the languages of choice for implementing analytical methods, so there are many tools you can explore after completing this book.\n",
    "\n",
    "In this chapter, I will review some features of pandas that may be helpful when you’re crossing back and forth between data wrangling with pandas and model fitting and scoring. I will then give short introductions to two popular modeling toolkits, stats-models and scikit-learn. Since each of these projects is large enough to warrant its own dedicated book, I make no effort to be comprehensive and instead direct you to both projects’ online documentation along with some other Python-based books on data science, statistics, and machine learning.\n",
    "\n",
    "# Interfacing Between pandas and Model Code\n",
    "\n",
    "A common workflow for model development is to use pandas for data loading and cleaning before switching over to a modeling library to build the model itself. An important part of the model development process is called feature engineering in machine learning. This can describe any data transformation or analytics that extract information from a raw dataset that may be useful in a modeling context. The data aggregation and GroupBy tools we have explored in this book are used often in a feature engineering context.\n",
    "\n",
    "While details of “good” feature engineering are out of scope for this book, I will show some methods to make switching between data manipulation with pandas and modeling as painless as possible.\n",
    "\n",
    "The point of contact between pandas and other analysis libraries is usually NumPy arrays. To turn a `DataFrame` into a NumPy array, use the .values property:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.DataFrame({\n",
    "    'x0': [1, 2, 3, 4, 5],\n",
    "    'x1': [0.01, -0.01, 0.25, -4.1, 0.],\n",
    "    'y': [-1.5, 0., 3.6, 1.3, -2.]})\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "   x0    x1   y\n",
    "0   1 0.01 -1.5\n",
    "1   2 -0.01 0.0\n",
    "2   3 0.25 3.6\n",
    "3   4 -4.10 1.3\n",
    "4   5 0.00 -2.0\n",
    "    `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`Index(['x0', 'x1', 'y'], dtype='object')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "array([[    1. , 0.01, -1.5          ],\n",
    "       [    2. , -0.01, 0.           ],\n",
    "       [    3. , 0.25, 3.6           ],\n",
    "       [    4. , -4.1 , 1.3          ],\n",
    "       [    5. , 0. , -2.            ]])\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert back to a `DataFrame`, as you may recall from earlier chapters, you can pass a two-dimensional ndarray with optional column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(data.values, columns=['one', 'two', 'three'])\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "   one   two       three\n",
    "0 1.0 0.01          -1.5\n",
    "1 2.0 -0.01          0.0\n",
    "2 3.0 0.25           3.6\n",
    "3 4.0 -4.10          1.3\n",
    "4 5.0 0.00          -2.0\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .values attribute is intended to be used when your data is homogeneous - for example, all numeric types. If you have heterogeneous data, the result will be an ndarray of Python objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = data.copy()\n",
    "df3['strings'] = ['a', 'b', 'c', 'd', 'e']\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "   x0    x1   y strings\n",
    "0   1 0.01 -1.5       a\n",
    "1   2 -0.01 0.0       b\n",
    "2   3 0.25 3.6        c\n",
    "3   4 -4.10 1.3       d\n",
    "4   5 0.00 -2.0       e\n",
    "    `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "array([[1, 0.01, -1.5, 'a'],\n",
    "       [2, -0.01, 0.0, 'b'],\n",
    "       [3, 0.25, 3.6, 'c'],\n",
    "       [4, -4.1, 1.3, 'd'],\n",
    "       [5, 0.0, -2.0, 'e']], dtype=object)\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some models, you may only wish to use a subset of the columns. I recommend using loc indexing with values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cols = ['x0', 'x1']\n",
    "data.loc[:, model_cols].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "array([[   1.    , 0.01],\n",
    "       [   2.    , -0.01],\n",
    "       [   3.    , 0.25],\n",
    "       [   4.    , -4.1 ],\n",
    "       [   5.    , 0. ]])\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some libraries have native support for pandas and do some of this work for you automatically: converting to NumPy from `DataFrame` and attaching model parameter names to the columns of output tables or `Series`. In other cases, you will have to perform this “metadata management” manually.\n",
    "\n",
    "In Chapter 12 we looked at pandas’s Categorical type and the pandas.get_dummies function. Suppose we had a non-numeric column in our example dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['category'] = pd.Categorical(['a', 'b', 'a', 'a', 'b'],\n",
    "                                  categories=['a', 'b'])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "   x0    x1   y category\n",
    "0   1 0.01 -1.5        a\n",
    "1   2 -0.01 0.0        b\n",
    "2   3 0.25 3.6         a\n",
    "3   4 -4.10 1.3        a\n",
    "4   5 0.00 -2.0        b\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to replace the 'category' column with dummy variables, we create dummy variables, drop the 'category' column, and then join the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = pd.get_dummies(data.category, prefix='category')\n",
    "data_with_dummies = data.drop('category', axis=1).join(dummies)\n",
    "data_with_dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "   x0    x1    y category_a category_b\n",
    "0   1 0.01 -1.5            1         0\n",
    "1   2 -0.01 0.0            0         1\n",
    "2   3 0.25 3.6             1         0\n",
    "3   4 -4.10 1.3            1         0\n",
    "4   5 0.00 -2.0            0         1\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some nuances to fitting certain statistical models with dummy variables. It may be simpler and less error-prone to use Patsy (the subject of the next section) when you have more than simple numeric columns.\n",
    "\n",
    "# Creating Model Descriptions with Patsy\n",
    "\n",
    "Patsy is a Python library for describing statistical models (especially linear models) with a small string-based “formula syntax,” which is inspired by (but not exactly the same as) the formula syntax used by the R and S statistical programming languages.\n",
    "\n",
    "Patsy is well supported for specifying linear models in statsmodels, so I will focus on some of the main features to help you get up and running. Patsy’s formulas are a special string syntax that looks like:\n",
    "\n",
    "`y ~ x0 + x1`\n",
    "\n",
    "The syntax a + b does not mean to add a to b, but rather that these are terms in the design matrix created for the model. The patsy.dmatrices function takes a formula string along with a dataset (which can be a DataFrame or a dict of arrays) and produces design matrices for a linear model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\n",
    "    'x0': [1, 2, 3, 4, 5],\n",
    "    'x1': [0.01, -0.01, 0.25, -4.1, 0.],\n",
    "    'y': [-1.5, 0., 3.6, 1.3, -2.]})\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "   x0    x1   y\n",
    "0   1 0.01 -1.5\n",
    "1   2 -0.01 0.0\n",
    "2   3 0.25 3.6\n",
    "3   4 -4.10 1.3\n",
    "4   5 0.00 -2.0\n",
    "    `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import patsy\n",
    "y, X = patsy.dmatrices('y ~ x0 + x1', data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "DesignMatrix with shape (5, 1)\n",
    "     y\n",
    "  -1.5\n",
    "   0.0\n",
    "   3.6\n",
    "   1.3\n",
    "  -2.0\n",
    "  Terms:\n",
    "    'y' (column 0)\n",
    "    `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "DesignMatrix with shape (5, 3)\n",
    "  Intercept x0      x1\n",
    "           1  1   0.01\n",
    "           1  2 -0.01\n",
    "           1  3   0.25\n",
    "           1  4 -4.10\n",
    "           1  5   0.00\n",
    "  Terms:\n",
    "    'Intercept' (column 0)\n",
    "    'x0' (column 1)\n",
    "    'x1' (column 2)\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These Patsy DesignMatrix instances are NumPy ndarrays with additional metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "array([[-1.5],\n",
    "       [ 0. ],\n",
    "       [ 3.6],\n",
    "       [ 1.3],\n",
    "       [-2. ]])\n",
    "    `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "array([[   1. , 1. , 0.01],\n",
    "       [   1. , 2. , -0.01],\n",
    "       [   1. , 3. , 0.25],\n",
    "       [   1. , 4. , -4.1 ],\n",
    "       [   1. , 5. , 0. ]])\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might wonder where the Intercept term came from. This is a convention for linear models like ordinary least squares (OLS) regression. You can suppress the intercept by adding the term + 0 to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patsy.dmatrices('y ~ x0 + x1 + 0', data)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "DesignMatrix with shape (5, 2)\n",
    "  x0      x1\n",
    "   1    0.01\n",
    "   2 -0.01\n",
    "   3    0.25\n",
    "   4 -4.10\n",
    "   5    0.00\n",
    "  Terms:\n",
    "     'x0' (column 0)\n",
    "     'x1' (column 1)\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Patsy objects can be passed directly into algorithms like numpy.linalg.lstsq, which performs an ordinary least squares regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef, resid, _, _ = np.linalg.lstsq(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model metadata is retained in the design_info attribute, so you can reattach the model column names to the fitted coefficients to obtain a Series, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "array([[ 0.3129],\n",
    "       [-0.0791],\n",
    "       [-0.2655]])\n",
    "    `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = pd.Series(coef.squeeze(), index=X.design_info.column_names)\n",
    "coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "Intercept     0.312910\n",
    "x0          -0.079106\n",
    "x1          -0.265464\n",
    "dtype: float64\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformations in Patsy Formulas\n",
    "\n",
    "You can mix Python code into your Patsy formulas; when evaluating the formula the library will try to find the functions you use in the enclosing scope:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, X = patsy.dmatrices('y ~ x0 + np.log(np.abs(x1) + 1)', data)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "DesignMatrix with shape (5, 3)\n",
    "  Intercept x0 np.log(np.abs(x1) + 1)\n",
    "          1   1                 0.00995\n",
    "          1   2                 0.00995\n",
    "          1   3                 0.22314\n",
    "          1   4                 1.62924\n",
    "          1   5                 0.00000\n",
    "  Terms:\n",
    "    'Intercept' (column 0)\n",
    "    'x0' (column 1)\n",
    "    'np.log(np.abs(x1) + 1)' (column 2)\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some commonly used variable transformations include standardizing (to mean 0 and variance 1) and centering (subtracting the mean). Patsy has built-in functions for this purpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, X = patsy.dmatrices('y ~ standardize(x0) + center(x1)', data)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "DesignMatrix with shape (5, 3)\n",
    "  Intercept standardize(x0) center(x1)\n",
    "           1        -1.41421      0.78\n",
    "           1        -0.70711      0.76\n",
    "           1         0.00000      1.02\n",
    "           1         0.70711     -3.33\n",
    "           1         1.41421      0.77\n",
    "  Terms:\n",
    "    'Intercept' (column 0)\n",
    "    'standardize(x0)' (column 1)\n",
    "    'center(x1)' (column 2)\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of a modeling process, you may fit a model on one dataset, then evaluate the model based on another. This might be a hold-out portion or new data that is observed later. When applying transformations like center and standardize, you should be careful when using the model to form predications based on new data. These are called stateful transformations, because you must use statistics like the mean or standard deviation of the original dataset when transforming a new dataset.\n",
    "\n",
    "The patsy.build_design_matrices function can apply transformations to new out-of-sample data using the saved information from the original in-sample dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.DataFrame({\n",
    "    'x0': [6, 7, 8, 9],\n",
    "    'x1': [3.1, -0.5, 0, 2.3],\n",
    "    'y': [1, 2, 3, 4]})\n",
    "new_X = patsy.build_design_matrices([X.design_info], new_data)\n",
    "new_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "[DesignMatrix with shape (4, 3)\n",
    "   Intercept standardize(x0) center(x1)\n",
    "           1          2.12132      3.87\n",
    "           1          2.82843      0.27\n",
    "           1          3.53553      0.77\n",
    "           1          4.24264      3.07\n",
    "   Terms:\n",
    "     'Intercept' (column 0)\n",
    "     'standardize(x0)' (column 1)\n",
    "     'center(x1)' (column 2)]\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the plus symbol (+) in the context of Patsy formulas does not mean addition, when you want to add columns from a dataset by name, you must wrap them in the special I function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, X = patsy.dmatrices('y ~ I(x0 + x1)', data)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "DesignMatrix with shape (5, 2)\n",
    "  Intercept I(x0 + x1)\n",
    "           1       1.01\n",
    "           1       1.99\n",
    "           1       3.25\n",
    "           1      -0.10\n",
    "           1       5.00\n",
    "  Terms:\n",
    "    'Intercept' (column 0)\n",
    "    'I(x0 + x1)' (column 1)\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patsy has several other built-in transforms in the patsy.builtins module. See the online documentation for more.\n",
    "\n",
    "Categorical data has a special class of transformations, which I explain next.\n",
    "\n",
    "# Categorical Data and Patsy\n",
    "\n",
    "Non-numeric data can be transformed for a model design matrix in many different ways. A complete treatment of this topic is outside the scope of this book and would be best studied along with a course in statistics.\n",
    "\n",
    "When you use non-numeric terms in a Patsy formula, they are converted to dummy variables by default. If there is an intercept, one of the levels will be left out to avoid collinearity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\n",
    "    'key1': ['a', 'a', 'b', 'b', 'a', 'b', 'a', 'b'],\n",
    "    'key2': [0, 1, 0, 1, 0, 1, 0, 0],\n",
    "    'v1': [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "    'v2': [-1, 0, 2.5, -0.5, 4.0, -1.2, 0.2, -1.7]\n",
    "})\n",
    "y, X = patsy.dmatrices('v2 ~ key1', data)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "DesignMatrix with shape (8, 2)\n",
    "  Intercept key1[T.b]\n",
    "           1         0\n",
    "           1         0\n",
    "           1         1\n",
    "           1          1\n",
    "           1          0\n",
    "           1          1\n",
    "           1          0\n",
    "           1          1\n",
    "  Terms:\n",
    "    'Intercept' (column 0)\n",
    "    'key1' (column 1)\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you omit the intercept from the model, then columns for each category value will be included in the model design matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, X = patsy.dmatrices('v2 ~ key1 + 0', data)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "DesignMatrix with shape (8, 2)\n",
    "  key1[a] key1[b]\n",
    "         1       0\n",
    "         1       0\n",
    "         0       1\n",
    "         0       1\n",
    "         1       0\n",
    "         0       1\n",
    "         1       0\n",
    "         0       1\n",
    "  Terms:\n",
    "    'key1' (columns 0:2)\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numeric columns can be interpreted as categorical with the C function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, X = patsy.dmatrices('v2 ~ C(key2)', data)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "DesignMatrix with shape (8, 2)\n",
    "  Intercept C(key2)[T.1]\n",
    "           1             0\n",
    "           1             1\n",
    "           1             0\n",
    "           1             1\n",
    "           1             0\n",
    "           1             1\n",
    "           1             0\n",
    "           1             0\n",
    "  Terms:\n",
    "    'Intercept' (column 0)\n",
    "    'C(key2)' (column 1)\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you’re using multiple categorical terms in a model, things can be more complicated, as you can include interaction terms of the form key1:key2, which can be used, for example, in analysis of variance (ANOVA) models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['key2'] = data['key2'].map({0: 'zero', 1: 'one'})\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "  key1 key2 v1    v2\n",
    "0    a zero   1 -1.0\n",
    "1    a   one  2 0.0\n",
    "2    b zero   3 2.5\n",
    "3    b   one  4 -0.5\n",
    "4    a zero   5 4.0\n",
    "5    b   one  6 -1.2\n",
    "6    a zero   7 0.2\n",
    "7    b zero   8 -1.7\n",
    "    `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, X = patsy.dmatrices('v2 ~ key1 + key2', data)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "DesignMatrix with shape (8, 3)\n",
    "  Intercept key1[T.b] key2[T.zero]\n",
    "           1         0           1\n",
    "           1         0           0\n",
    "           1         1           1\n",
    "           1         1           0\n",
    "           1         0           1\n",
    "           1         1           0\n",
    "           1         0           1\n",
    "           1         1           1\n",
    "  Terms:\n",
    "    'Intercept' (column 0)\n",
    "    'key1' (column 1)\n",
    "    'key2' (column 2)\n",
    "    `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, X = patsy.dmatrices('v2 ~ key1 + key2 + key1:key2', data)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "DesignMatrix with shape   (8, 4)\n",
    "  Intercept key1[T.b]     key2[T.zero] key1[T.b]:key2[T.zero]\n",
    "           1         0               1                      0\n",
    "           1         0               0                      0\n",
    "           1         1               1                      1\n",
    "           1         1               0                      0\n",
    "           1         0               1                      0\n",
    "           1         1               0                      0\n",
    "           1         0               1                      0\n",
    "           1         1               1                      1\n",
    "  Terms:\n",
    "    'Intercept' (column   0)\n",
    "    'key1' (column 1)\n",
    "    'key2' (column 2)\n",
    "    'key1:key2' (column   3)\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patsy provides for other ways to transform categorical data, including transformations for terms with a particular ordering. See the online documentation for more.\n",
    "\n",
    "# Introduction to statsmodels\n",
    "\n",
    "statsmodels is a Python library for fitting many kinds of statistical models, performing statistical tests, and data exploration and visualization. Statsmodels contains more “classical” frequentist statistical methods, while Bayesian methods and machine learning models are found in other libraries.\n",
    "\n",
    "Some kinds of models found in statsmodels include:\n",
    "\n",
    "- Linear models, generalized linear models, and robust linear models\n",
    "- Linear mixed effects models\n",
    "- Analysis of variance (ANOVA) methods\n",
    "- Time series processes and state space models\n",
    "- Generalized method of moments\n",
    "\n",
    "In the next few pages, we will use a few basic tools in statsmodels and explore how to use the modeling interfaces with Patsy formulas and pandas DataFrame objects.\n",
    "\n",
    "# Estimating Linear Models\n",
    "\n",
    "There are several kinds of linear regression models in statsmodels, from the more basic (e.g., ordinary least squares) to more complex (e.g., iteratively reweighted least squares).\n",
    "\n",
    "Linear models in statsmodels have two different main interfaces: array-based and formula-based. These are accessed through these API module imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show how to use these, we generate a linear model from some random data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dnorm(mean, variance, size=1):\n",
    "    if isinstance(size, int):\n",
    "        size = size,\n",
    "    return mean + np.sqrt(variance) * np.random.randn(*size)\n",
    "# For reproducibility\n",
    "np.random.seed(12345)\n",
    "N = 100\n",
    "X = np.c_[dnorm(0, 0.4, size=N),\n",
    "          dnorm(0, 0.6, size=N),\n",
    "          dnorm(0, 0.2, size=N)]\n",
    "eps = dnorm(0, 0.1, size=N)\n",
    "beta = [0.1, 0.3, 0.5]\n",
    "y = np.dot(X, beta) + eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I wrote down the “true” model with known parameters beta. In this case, dnorm is a helper function for generating normally distributed data with a particular mean and variance. So now we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "array([[-0.1295,       -1.2128, 0.5042],\n",
    "       [ 0.3029,       -0.4357, -0.2542],\n",
    "       [-0.3285,       -0.0253, 0.1384],\n",
    "       [-0.3515,       -0.7196, -0.2582],\n",
    "       [ 1.2433,       -0.3738, -0.5226]])\n",
    "    `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`array([ 0.4279, -0.6735, -0.0909, -0.4895, -0.1289])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear model is generally fitted with an intercept term as we saw before with Patsy. The sm.add_constant function can add an intercept column to an existing matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_model = sm.add_constant(X)\n",
    "X_model[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "array([[    1.      , -0.1295, -1.2128, 0.5042],\n",
    "       [    1.      , 0.3029, -0.4357, -0.2542],\n",
    "       [    1.      , -0.3285, -0.0253, 0.1384],\n",
    "       [    1.      , -0.3515, -0.7196, -0.2582],\n",
    "       [    1.      , 1.2433, -0.3738, -0.5226]])\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sm.OLS class can fit an ordinary least squares linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(y, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model’s fit method returns a regression results object containing estimated model parameters and other diagnostics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.fit()\n",
    "results.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`array([ 0.1783,   0.223 ,   0.501 ])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summary method on results can print a model detailing diagnostic output of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "OLS Regression Results\n",
    "\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\n",
    "Dep. Variable:                        y   R-squared:                     0.430\n",
    "Model:                              OLS   Adj. R-squared:                0.413\n",
    "Method:                  Least Squares    F-statistic:                   24.42\n",
    "Date:                 Mon, 25 Sep 2017    Prob (F-statistic):         7.44e-12\n",
    "Time:                         14:06:15    Log-Likelihood:              -34.305\n",
    "No. Observations:                   100   AIC:                           74.61\n",
    "Df Residuals:                        97   BIC:                           82.42\n",
    "Df Model:                             3\n",
    "Covariance Type:             nonrobust\n",
    "\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\n",
    "                  coef    std err           t       P>|t|     [0.025    0.975]\n",
    "------------------------------------------------------------------------------\n",
    "x1             0.1783       0.053       3.364       0.001      0.073     0.283\n",
    "x2             0.2230       0.046       4.818       0.000      0.131     0.315\n",
    "x3             0.5010       0.080       6.237       0.000      0.342     0.660\n",
    "\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\n",
    "Omnibus:                          4.662   Durbin-Watson:                 2.201\n",
    "Prob(Omnibus):                    0.097   Jarque-Bera (JB):              4.098\n",
    "Skew:                             0.481   Prob(JB):                      0.129\n",
    "Kurtosis:                         3.243   Cond. No.                       1.74\n",
    "\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\\=\n",
    "Warnings:\n",
    "[1] Standard Errors assume that the covariance matrix of the errors is correctly\n",
    "specified.\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter names here have been given the generic names x1, x2, and so on. Suppose instead that all of the model parameters are in a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(X, columns=['col0', 'col1', 'col2'])\n",
    "data['y'] = y\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "     col0       col1      col2         y\n",
    "0 -0.129468 -1.212753 0.504225 0.427863\n",
    "1 0.302910 -0.435742 -0.254180 -0.673480\n",
    "2 -0.328522 -0.025302 0.138351 -0.090878\n",
    "3 -0.351475 -0.719605 -0.258215 -0.489494\n",
    "4 1.243269 -0.373799 -0.522629 -0.128941\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the statsmodels formula API and Patsy formula strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = smf.ols('y ~ col0 + col1 + col2', data=data).fit()\n",
    "results.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "Intercept    0.033559\n",
    "col0         0.176149\n",
    "col1         0.224826\n",
    "col2         0.514808\n",
    "dtype: float64\n",
    "    `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.tvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "Intercept    0.952188\n",
    "col0         3.319754\n",
    "col1         4.850730\n",
    "col2         6.303971\n",
    "dtype: float64\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe how statsmodels has returned results as Series with the DataFrame column names attached. We also do not need to use add_constant when using formulas and pandas objects.\n",
    "\n",
    "Given new out-of-sample data, you can compute predicted values given the estimated model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.predict(data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "0   -0.002327\n",
    "1   -0.141904\n",
    "2    0.041226\n",
    "3   -0.323070\n",
    "4   -0.100535\n",
    "dtype: float64\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many additional tools for analysis, diagnostics, and visualization of linear model results in statsmodels that you can explore. There are also other kinds of linear models beyond ordinary least squares.\n",
    "\n",
    "# Estimating Time Series Processes\n",
    "\n",
    "Another class of models in statsmodels are for time series analysis. Among these are autoregressive processes, Kalman filtering and other state space models, and multivariate autoregressive models.\n",
    "\n",
    "Let’s simulate some time series data with an autoregressive structure and noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_x = 4\n",
    "import random\n",
    "values = [init_x, init_x]\n",
    "N = 1000\n",
    "b0 = 0.8\n",
    "b1 = -0.4\n",
    "noise = dnorm(0, 0.1, N)\n",
    "for i in range(N):\n",
    "    new_x = values[-1] * b0 + values[-2] * b1 + noise[i]\n",
    "    values.append(new_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data has an AR(2) structure (two lags) with parameters 0.8 and -0.4. When you fit an AR model, you may not know the number of lagged terms to include, so you can fit the model with some larger number of lags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXLAGS = 5\n",
    "model = sm.tsa.AR(values)\n",
    "results = model.fit(MAXLAGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated parameters in the results have the intercept first and the estimates for the first two lags next:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "array([-0.0062,    0.7845, -0.4085, -0.0136,     0.015 ,      0.0143])\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deeper details of these models and how to interpret their results is beyond what i can cover in this book, but there’s plenty more to discover in the statsmodels documentation.\n",
    "\n",
    "# Introduction to scikit-learn\n",
    "\n",
    "scikit-learn is one of the most widely used and trusted general-purpose Python machine learning toolkits. It contains a broad selection of standard supervised and unsupervised machine learning methods with tools for model selection and evaluation, data transformation, data loading, and model persistence. These models can be used for classification, clustering, prediction, and other common tasks.\n",
    "\n",
    "There are excellent online and printed resources for learning about machine learning and how to apply libraries like scikit-learn and TensorFlow to solve real-world problems. In this section, I will give a brief flavor of the scikit-learn API style.\n",
    "\n",
    "At the time of this writing, scikit-learn does not have deep pandas integration, though there are some add-on third-party packages that are still in development. pandas can be very useful for massaging datasets prior to model fitting, though.\n",
    "\n",
    "As an example, I use a now-classic dataset from a Kaggle competition about passenger survival rates on the Titanic, which sank in 1912. We load the test and training dataset using pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('datasets/titanic/train.csv')\n",
    "test = pd.read_csv('datasets/titanic/test.csv')\n",
    "train[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "   PassengerId Survived            Pclass 0            1        0                 3\n",
    "1            2        1                 1\n",
    "2            3        1                 3\n",
    "3            4        1                 1\n",
    "                                                Name    Sex  Age SibSp 0                            Braund, Mr. Owen Harris   male 22.0     1\n",
    "1 Cumings, Mrs. John Bradley (Florence Briggs Th... female 38.0      1\n",
    "2                             Heikkinen, Miss. Laina female 26.0     0\n",
    "3       Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.0     1\n",
    "  Parch             Ticket     Fare Cabin Embarked\n",
    "0     0          A/5 21171   7.2500   NaN        S\n",
    "1     0           PC 17599 71.2833    C85        C\n",
    "2     0 STON/O2. 3101282     7.9250   NaN        S\n",
    "3     0             113803 53.1000 C123          S\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries like statsmodels and scikit-learn generally cannot be fed missing data, so we ook at the columns to see if there are any that contain missing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "PassengerId      0\n",
    "Survived         0\n",
    "Pclass           0\n",
    "Name             0\n",
    "Sex              0\n",
    "Age            177\n",
    "SibSp            0\n",
    "Parch            0\n",
    "Ticket           0\n",
    "Fare             0\n",
    "Cabin          687\n",
    "Embarked         2\n",
    "dtype: int64\n",
    "    `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "PassengerId      0\n",
    "Pclass           0\n",
    "Name             0\n",
    "Sex              0\n",
    "Age             86\n",
    "SibSp               0\n",
    "Parch               0\n",
    "Ticket              0\n",
    "Fare                1\n",
    "Cabin             327\n",
    "Embarked            0\n",
    "dtype: int64\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In statistics and machine learning examples like this one, a typical task is to predict hether a passenger would survive based on features in the data. A model is fitted on  training dataset and then evaluated on an out-of-sample testing dataset.\n",
    "\n",
    "I would like to use Age as a predictor, but it has missing data. There are a number of ays to do missing data imputation, but I will do a simple one and use the median of he training dataset to fill the nulls in both tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_value = train['Age'].median()\n",
    "train['Age'] = train['Age'].fillna(impute_value)\n",
    "test['Age'] = test['Age'].fillna(impute_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to specify our models. I add a column IsFemale as an encoded version f the 'Sex' column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['IsFemale'] = (train['Sex'] == 'female').astype(int)\n",
    "test['IsFemale'] = (test['Sex'] == 'female').astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we decide on some model variables and create NumPy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = ['Pclass', 'IsFemale', 'Age']\n",
    "X_train = train[predictors].values\n",
    "X_test = test[predictors].values\n",
    "y_train = train['Survived'].values\n",
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "array([[    3.,    0., 22.],\n",
    "       [    1.,    1., 38.],\n",
    "       [    3.,    1., 26.],\n",
    "       [    1.,    1., 35.],\n",
    "       [    3.,    0., 35.]])\n",
    "    `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`array([0, 1, 1, 1, 0])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I make no claims that this is a good model nor that these features are engineered roperly. We use the LogisticRegression model from scikit-learn and create a odel instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to statsmodels, we can fit this model to the training data using the model’s fit ethod:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
    "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
    "          verbose=0, warm_start=False)\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can form predictions for the test dataset using model.predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = model.predict(X_test)\n",
    "y_predict[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`array([0, 0, 0, 0, 1, 0, 1, 0, 1, 0])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f you had the true values for the test dataset, you could compute an accuracy percentage or some other error metric:\n",
    "\n",
    "`(y_true == y_predict).mean()`\n",
    "\n",
    "In practice, there are often many additional layers of complexity in model training. Many models have parameters that can be tuned, and there are techniques such as cross-validation that can be used for parameter tuning to avoid overfitting to the training data. This can often yield better predictive performance or robustness on new data.\n",
    "\n",
    "Cross-validation works by splitting the training data to simulate out-of-sample prediction. Based on a model accuracy score like mean squared error, one can perform a grid search on model parameters. Some models, like logistic regression, have estimator classes with built-in cross-validation. For example, the LogisticRegressionCV class can be used with a parameter indicating how fine-grained of a grid search to do on the model regularization parameter C:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "model_cv = LogisticRegressionCV(10)\n",
    "model_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "LogisticRegressionCV(Cs=10, class_weight=None, cv=None, dual=False,\n",
    "           fit_intercept=True, intercept_scaling=1.0, max_iter=100,\n",
    "           multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
    "           refit=True, scoring=None, solver='lbfgs', tol=0.0001, verbose=0)\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do cross-validation by hand, you can use the cross_val_score helper function, which handles the data splitting process. For example, to cross-validate our model with four non-overlapping splits of the training data, we can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "model = LogisticRegression(C=10)\n",
    "scores = cross_val_score(model, X_train, y_train, cv=4)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`array([ 0.7723,   0.8027,   0.7703,   0.7883])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default scoring metric is model-dependent, but it is possible to choose an explicit scoring function. Cross-validated models take longer to train, but can often yield better model performance.\n",
    "\n",
    "# Continuing Your Education\n",
    "\n",
    "While I have only skimmed the surface of some Python modeling libraries, there are more and more frameworks for various kinds of statistics and machine learning either implemented in Python or with a Python user interface.\n",
    "\n",
    "This book is focused especially on data wrangling, but there are many others dedicated to modeling and data science tools. Some excellent ones are:\n",
    "\n",
    "- _Introduction to Machine Learning with Python_ by Andreas Mueller and Sarah Guido (O’Reilly)\n",
    "- _Python Data Science Handbook_ by Jake VanderPlas (O’Reilly)\n",
    "- _Data Science from Scratch: First Principles with Python_ by Joel Grus (O’Reilly)\n",
    "- _Python Machine Learning_ by Sebastian Raschka (Packt Publishing)\n",
    "- _Hands-On Machine Learning with Scikit-Learn and TensorFlow_ by Aurélien Géron (O’Reilly)\n",
    "\n",
    "While books can be valuable resources for learning, they can sometimes grow out of date when the underlying open source software changes. It’s a good idea to be familiar with the documentation for the various statistics or machine learning frameworks to stay up to date on the latest features and API."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
