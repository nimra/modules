{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ArtificialGeneralIntelligence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are widespread discussions about whether artificial general intelligence (AGI) will soon come into existence. Experts disagree strongly over whether AGI is worth seriously planning for. Our view is that while there’s no harm in doing research on “AI value alignment” and “safe reward function” design, the artificial intelligence systems of today and the foreseeable future are unlikely to rapidly achieve sentience. As you will have learned first hand, most deep learning systems are simply sophisticated numerical engines, prone to many finicky numerical stability issues. It will likely take decades of fundamental advances before general intelligence becomes an issue. At the same time, as we’ve discussed in the previous section, artificial intelligence is already having dramatic impact on human societies and industries. It is absolutely worth worrying about the effects of AI without the need to conjure superintelligent bogeymen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid black; padding: 10px;\"><b style=\"font-size: 2em;\">The Superintelligent Fallacy</b><br> The book Superintelligence by Nick Bostrom (Oxford University Press) has had a profound impact upon the discourse surrounding AI. The basic premise of the book is that an intelligence explosion could occur when models become capable of recursively improving themselves. In itself, the premise of the book isn’t that radical. If AGI were to come into existence, there’s no reason to suppose that it couldn’t succeed in improving itself rapidly.<br>At the same time, deep learning expert Andrew Ng has gone on the record stating that worrying about superintelligence is like worrying about overpopulation on Mars. One day, humanity is likely to reach Mars. When enough people land on Mars, overcrowding will likely exist and may even be a very serious problem. None of this changes the fact that Mars today is an empty wasteland. So too is the state of the literature on creating generally intelligent AI!<br>Now, this last statement is hyperbolic. Solid progress in reinforcement learning and generative modeling holds much promise for creating more intelligent agents. But, stressing over the possibilities for superintelligent entities detracts from the very real challenges of automation coming our way. Of course, this doesn’t even mention other serious challenges facing us, such as global warming.</div>"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
