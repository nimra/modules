{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomPatchesSubspaces\n",
    "\n",
    "The `BaggingClassifier` class supports sampling the features as well. This is controlled by two hyperparameters: `max_features` and `bootstrap_features`. They work the same way as `max_samples` and `bootstrap`, but for feature sampling instead of instance sampling. Thus, each predictor will be trained on a random subset of the input features.\n",
    "\n",
    "This is particularly useful when you are dealing with high-dimensional inputs (such as images). Sampling both training instances and features is called the [Random Patches](https://link.springer.com/chapter/10.1007/978-3-642-33460-3_28) method.7 Keeping all training instances (i.e., `bootstrap=False` and `max_samples=1.0`) but sampling features (i.e., `bootstrap_features=True` and/or `max_features` smaller than 1.0) is called the [Random Subspaces](https://ieeexplore.ieee.org/document/709601?tp=&arnumber=709601&url=http:%2F%2Fieeexplore.ieee.org%2Fiel4%2F34%2F15364%2F00709601.pdf) method.\n",
    "\n",
    "> **_Note_**<br>\n",
    "- <b>7</b> | “Ensembles on Random Patches,” G. Louppe and P. Geurts (2012). <br />\n",
    "- <b>8</b> | “The random subspace method for constructing decision forests,” Tin Kam Ho (1998).\n",
    "\n",
    "Sampling features results in even more predictor diversity, trading a bit more bias for a lower variance."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
