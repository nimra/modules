{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Preparation\n",
    "\n",
    "During the course of doing data analysis and modeling, a significant amount of time is spent on data preparation: loading, cleaning, transforming, and rearranging. Such tasks are often reported to take up 80% or more of an analyst’s time. Sometimes the way that data is stored in files or databases is not in the right format for a particular task. Many researchers choose to do ad hoc processing of data from one form to another using a general-purpose programming language, like Python, Perl, R, or Java, or Unix text-processing tools like sed or awk. Fortunately, pandas, along with the built-in Python language features, provides you with a high-level, flexible, and fast set of tools to enable you to manipulate data into the right form.\n",
    "\n",
    "If you identify a type of data manipulation that isn’t anywhere in this book or elsewhere in the pandas library, feel free to share your use case on one of the Python mailing lists or on the pandas GitHub site. Indeed, much of the design and implementation of pandas has been driven by the needs of real-world applications.\n",
    "\n",
    "In this chapter I discuss tools for missing data, duplicate data, string manipulation, and some other analytical data transformations. In the next chapter, I focus on combining and rearranging datasets in various ways.\n",
    "\n",
    "# Handling Missing Data\n",
    "\n",
    "Missing data occurs commonly in many data analysis applications. One of the goals of pandas is to make working with missing data as painless as possible. For example, all of the descriptive statistics on pandas objects exclude missing data by default.\n",
    "\n",
    "The way that missing data is represented in pandas objects is somewhat imperfect, but it is functional for a lot of users. For numeric data, pandas uses the floating-point value NaN (Not a Number) to represent missing data. We call this a sentinel value that can be easily detected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_data = pd.Series(['aardvark', 'artichoke', np.nan, 'avocado'])\n",
    "string_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "0     aardvark\n",
    "1    artichoke\n",
    "2          NaN\n",
    "3      avocado\n",
    "dtype: object\n",
    "    `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_data.isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "0    False\n",
    "1    False\n",
    "2     True\n",
    "3    False\n",
    "dtype: bool\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pandas, we’ve adopted a convention used in the R programming language by referring to missing data as NA, which stands for not available. In statistics applications, NA data may either be data that does not exist or that exists but was not observed (through problems with data collection, for example). When cleaning up data for analysis, it is often important to do analysis on the missing data itself to identify data collection problems or potential biases in the data caused by missing data.\n",
    "\n",
    "The built-in Python None value is also treated as NA in object arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_data[0] = None\n",
    "string_data.isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "0     True\n",
    "1    False\n",
    "2     True\n",
    "3    False\n",
    "dtype: bool\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is work ongoing in the pandas project to improve the internal details of how missing data is handled, but the user API functions, like pandas.isnull, abstract away many of the annoying details. See Table 7-1 for a list of some functions related to missing data handling.\n",
    "\n",
    "_Table 7-1. NA handling methods_\n",
    "\n",
    "| Argument | Description |\n",
    "|----------|-----------------------------------------|\n",
    "| `dropna`  | Filter axis labels based on whether values for each label have missing data, with varying thresholds for how much missing data to tolerate. |\n",
    "| `fillna`  | Fill in missing data with some value or using an interpolation method such as 'ffill' or 'bfill'. |\n",
    "| `isnull`  | Return boolean values indicating which values are missing/NA. |\n",
    "| `notnull` | Negation of isnull. |\n",
    "\n",
    "# Filtering Out Missing Data\n",
    "\n",
    "There are a few ways to filter out missing data. While you always have the option to do it by hand using pandas.isnull and boolean indexing, the dropna can be helpful. On a Series, it returns the Series with only the non-null data and index values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import nan as NA\n",
    "data = pd.Series([1, NA, 3.5, NA, 7])\n",
    "data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "0    1.0\n",
    "2    3.5\n",
    "4    7.0\n",
    "dtype: float64\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is equivalent to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "0    1.0\n",
    "2    3.5\n",
    "4    7.0\n",
    "dtype: float64\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With DataFrame objects, things are a bit more complex. You may want to drop rows or columns that are all NA or only those containing any NAs. dropna by default drops any row containing a missing value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(\n",
    "    [[1., 6.5, 3.],\n",
    "    [1., NA, NA],\n",
    "    [NA, NA, NA],\n",
    "    [NA, 6.5, 3.],\n",
    "])\n",
    "cleaned = data.dropna()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "   0    1   2\n",
    "0 1.0 6.5 3.0\n",
    "1 1.0 NaN NaN\n",
    "2 NaN NaN NaN\n",
    "3 NaN 6.5 3.0\n",
    "    `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "  0    1    2\n",
    "0 1.0 6.5 3.0\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing how='all' will only drop rows that are all NA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(how='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "         0    1    2\n",
    "0   1.0   6.5    3.0\n",
    "1   1.0   NaN    NaN\n",
    "3   NaN   6.5    3.0\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To drop columns in the same way, pass axis=1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[4] = NA\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "     0    1   2          4\n",
    "0 1.0 6.5 3.0          NaN\n",
    "1 1.0 NaN NaN          NaN\n",
    "2 NaN NaN NaN          NaN\n",
    "3 NaN 6.5 3.0          NaN\n",
    "    `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(axis=1, how='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "  0    1    2\n",
    "0 1.0 6.5 3.0\n",
    "1 1.0 NaN NaN\n",
    "2 NaN NaN NaN\n",
    "3 NaN 6.5 3.0\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A related way to filter out DataFrame rows tends to concern time series data. Suppose you want to keep only rows containing a certain number of observations. You can indicate this with the thresh argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.randn(7, 3))\n",
    "df.iloc[:4, 1] = NA\n",
    "df.iloc[:2, 2] = NA\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "          0        1         2\n",
    "0 -0.204708      NaN       NaN\n",
    "1 -0.555730      NaN       NaN\n",
    "2 0.092908       NaN 0.769023\n",
    "3 1.246435       NaN -1.296221\n",
    "4 0.274992 0.228913 1.352917\n",
    "5 0.886429 -2.001637 -0.371843\n",
    "6 1.669025 -0.438570 -0.539741\n",
    "    `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "          0          1       2\n",
    "4 0.274992 0.228913 1.352917\n",
    "5 0.886429 -2.001637 -0.371843\n",
    "6 1.669025 -0.438570 -0.539741\n",
    "    `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(thresh=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "           0         1         2\n",
    "2   0.092908       NaN 0.769023\n",
    "3   1.246435       NaN -1.296221\n",
    "4   0.274992 0.228913 1.352917\n",
    "5   0.886429 -2.001637 -0.371843\n",
    "6   1.669025 -0.438570 -0.539741\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filling In Missing Data\n",
    "\n",
    "Rather than filtering out missing data (and potentially discarding other data along with it), you may want to fill in the “holes” in any number of ways. For most purposes, the fillna method is the workhorse function to use. Calling fillna with a constant replaces missing values with that value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "         0         1        2\n",
    "0 -0.204708 0.000000 0.000000\n",
    "1 -0.555730 0.000000 0.000000\n",
    "2 0.092908 0.000000 0.769023\n",
    "3 1.246435 0.000000 -1.296221\n",
    "4 0.274992 0.228913 1.352917\n",
    "5 0.886429 -2.001637 -0.371843\n",
    "6 1.669025 -0.438570 -0.539741\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling fillna with a dict, you can use a different fill value for each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna({1: 0.5, 2: 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "        0         1         2\n",
    "0 -0.204708 0.500000 0.000000\n",
    "1 -0.555730 0.500000 0.000000\n",
    "2 0.092908 0.500000 0.769023\n",
    "3 1.246435 0.500000 -1.296221\n",
    "4 0.274992 0.228913 1.352917\n",
    "5 0.886429 -2.001637 -0.371843\n",
    "6 1.669025 -0.438570 -0.539741\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fillna returns a new object, but you can modify the existing object in-place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = df.fillna(0, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "         0        1         2\n",
    "0 -0.204708 0.000000 0.000000\n",
    "1 -0.555730 0.000000 0.000000\n",
    "2 0.092908 0.000000 0.769023\n",
    "3 1.246435 0.000000 -1.296221\n",
    "4 0.274992 0.228913 1.352917\n",
    "5 0.886429 -2.001637 -0.371843\n",
    "6 1.669025 -0.438570 -0.539741\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same interpolation methods available for reindexing can be used with fillna:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.randn(6, 3))\n",
    "df.iloc[2:, 1] = NA\n",
    "df.iloc[4:, 2] = NA\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "          0             1         2\n",
    "0 0.476985       3.248944 -1.021228\n",
    "1 -0.577087      0.124121 0.302614\n",
    "2 0.523772            NaN 1.343810\n",
    "3 -0.713544           NaN -2.370232\n",
    "4 -1.860761           NaN       NaN\n",
    "5 -1.265934           NaN       NaN\n",
    "    `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "        0         1         2\n",
    "0 0.476985 3.248944 -1.021228\n",
    "1 -0.577087 0.124121 0.302614\n",
    "2 0.523772 0.124121 1.343810\n",
    "3 -0.713544 0.124121 -2.370232\n",
    "4 -1.860761 0.124121 -2.370232\n",
    "5 -1.265934 0.124121 -2.370232\n",
    "    `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(method='ffill', limit=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "        0         1         2\n",
    "0 0.476985 3.248944 -1.021228\n",
    "1 -0.577087 0.124121 0.302614\n",
    "2 0.523772 0.124121 1.343810\n",
    "3 -0.713544 0.124121 -2.370232\n",
    "4 -1.860761       NaN -2.370232\n",
    "5 -1.265934       NaN -2.370232\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With fillna you can do lots of other things with a little creativity. For example, you might pass the mean or median value of a Series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.Series([1., NA, 3.5, NA, 7])\n",
    "data.fillna(data.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "0    1.000000\n",
    "1    3.833333\n",
    "2    3.500000\n",
    "3    3.833333\n",
    "4    7.000000\n",
    "dtype: float64\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See Table 7-2 for a reference on fillna.\n",
    "\n",
    "_Table 7-2. fillna function arguments_\n",
    "\n",
    "| Argument | Description |\n",
    "|----------|------------------------------------------|\n",
    "| `value`    | Scalar value or dict-like object to use to fill missing values |\n",
    "| `method`   | Interpolation; by default 'ffill' if function called with no other arguments|\n",
    "| `axis`     | Axis to fill on; default axis=0 |\n",
    "| `inplace`  | Modify the calling object without producing a copy |\n",
    "| `limit`    | For forward and backward filling, maximum number of consecutive periods to fill |\n",
    "\n",
    "# Data Transformation\n",
    "\n",
    "So far in this chapter we’ve been concerned with rearranging data. Filtering, cleaning, and other transformations are another class of important operations.\n",
    "\n",
    "# Removing Duplicates\n",
    "\n",
    "Duplicate rows may be found in a DataFrame for any number of reasons. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\n",
    "    'k1': ['one', 'two'] * 3 + ['two'],\n",
    "    'k2': [1, 1, 2, 3, 3, 4, 4],\n",
    "})\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "     k1 k2\n",
    "0 one    1\n",
    "1 two    1\n",
    "2 one    2\n",
    "3 two    3\n",
    "4 one    3\n",
    "5 two    4\n",
    "6 two    4\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataFrame method duplicated returns a boolean Series indicating whether each row is a duplicate (has been observed in a previous row) or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.duplicated()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "0    False\n",
    "1    False\n",
    "2    False\n",
    "3    False\n",
    "4    False\n",
    "5    False\n",
    "6     True\n",
    "dtype: bool\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relatedly, drop_duplicates returns a DataFrame where the duplicated array is False:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "     k1 k2\n",
    "0 one    1\n",
    "1 two    1\n",
    "2 one    2\n",
    "3 two    3\n",
    "4 one    3\n",
    "5 two    4\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of these methods by default consider all of the columns; alternatively, you can specify any subset of them to detect duplicates. Suppose we had an additional column of values and wanted to filter duplicates only based on the 'k1' column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['v1'] = range(7)\n",
    "data.drop_duplicates(['k1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "      k1 k2 v1\n",
    "0 one    1   0\n",
    "1 two    1   1\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "duplicated and drop_duplicates by default keep the first observed value combination. Passing keep='last' will return the last one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates(['k1', 'k2'], keep='last')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "      k1 k2 v1\n",
    "0 one    1   0\n",
    "1 two    1   1\n",
    "2 one    2   2\n",
    "3 two    3   3\n",
    "4 one    3   4\n",
    "6 two    4   6\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming Data Using a Function or Mapping\n",
    "\n",
    "For many datasets, you may wish to perform some transformation based on the values in an array, Series, or column in a DataFrame. Consider the following hypothetical data collected about various kinds of meat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'food': ['bacon', 'pulled pork', 'bacon',\n",
    "                              'Pastrami', 'corned beef', 'Bacon',\n",
    "                              'pastrami', 'honey ham', 'nova lox'],\n",
    "                     'ounces': [4, 3, 12, 6, 7.5, 8, 3, 5, 6]})\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "          food ounces\n",
    "0        bacon    4.0\n",
    "1 pulled pork     3.0\n",
    "2        bacon   12.0\n",
    "3      Pastrami     6.0\n",
    "4   corned beef     7.5\n",
    "5         Bacon     8.0\n",
    "6      pastrami     3.0\n",
    "7     honey ham     5.0\n",
    "8      nova lox     6.0\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you wanted to add a column indicating the type of animal that each food came from. Let’s write down a mapping of each distinct meat type to the kind of animal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meat_to_animal = {\n",
    "    'bacon': 'pig',\n",
    "    'pulled pork': 'pig',\n",
    "    'pastrami': 'cow',\n",
    "    'corned beef': 'cow',\n",
    "    'honey ham': 'pig',\n",
    "    'nova lox': 'salmon'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The map method on a Series accepts a function or dict-like object containing a mapping, but here we have a small problem in that some of the meats are capitalized and others are not. Thus, we need to convert each value to lowercase using the str.lower Series method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowercased = data['food'].str.lower()\n",
    "lowercased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "0           bacon\n",
    "1    pulled pork\n",
    "2           bacon\n",
    "3        pastrami\n",
    "4    corned beef\n",
    "5           bacon\n",
    "6        pastrami\n",
    "7      honey ham\n",
    "8        nova lox\n",
    "Name: food, dtype: object\n",
    "    `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['animal'] = lowercased.map(meat_to_animal)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "          food    ounces   animal\n",
    "0        bacon       4.0      pig\n",
    "1 pulled pork        3.0      pig\n",
    "2        bacon      12.0      pig\n",
    "3     Pastrami       6.0      cow\n",
    "4 corned beef        7.5      cow\n",
    "5        Bacon       8.0      pig\n",
    "6     pastrami       3.0      cow\n",
    "7    honey ham          5.0       pig\n",
    "8     nova lox          6.0    salmon\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also have passed a function that does all the work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['food'].map(lambda x: meat_to_animal[x.lower()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "0        pig\n",
    "1        pig\n",
    "2        pig\n",
    "3        cow\n",
    "4        cow\n",
    "5        pig\n",
    "6        cow\n",
    "7        pig\n",
    "8    salmon\n",
    "Name: food, dtype: object\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using map is a convenient way to perform element-wise transformations and other data cleaning-related operations.\n",
    "\n",
    "# Replacing Values\n",
    "\n",
    "Filling in missing data with the fillna method is a special case of more general value replacement. As you’ve already seen, map can be used to modify a subset of values in an object but replace provides a simpler and more flexible way to do so. Let’s consider this Series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.Series([1., -999., 2., -999., -1000., 3.])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "0        1.0\n",
    "1    -999.0\n",
    "2        2.0\n",
    "3    -999.0\n",
    "4   -1000.0\n",
    "5        3.0\n",
    "dtype: float64\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The -999 values might be sentinel values for missing data. To replace these with NA values that pandas understands, we can use replace, producing a new Series (unless you pass inplace=True):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.replace(-999, np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "0        1.0\n",
    "1        NaN\n",
    "2        2.0\n",
    "3        NaN\n",
    "4   -1000.0\n",
    "5       3.0\n",
    "dtype: float64\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to replace multiple values at once, you instead pass a list and then the substitute value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.replace([-999, -1000], np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "0    1.0\n",
    "1    NaN\n",
    "2    2.0\n",
    "3    NaN\n",
    "4    NaN\n",
    "5    3.0\n",
    "dtype: float64\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use a different replacement for each value, pass a list of substitutes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.replace([-999, -1000], [np.nan, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "0    1.0\n",
    "1    NaN\n",
    "2    2.0\n",
    "3    NaN\n",
    "4    0.0\n",
    "5    3.0\n",
    "dtype: float64\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The argument passed can also be a dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.replace({-999: np.nan, -1000: 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "0    1.0\n",
    "1    NaN\n",
    "2    2.0\n",
    "3    NaN\n",
    "4    0.0\n",
    "5    3.0\n",
    "dtype: float64\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_Note_**<br>The data.replace method is distinct from data.str.replace, which performs string substitution element-wise. We look at these string methods on Series later in the chapter.\n",
    "\n",
    "# Renaming Axis Indexes\n",
    "\n",
    "Like values in a Series, axis labels can be similarly transformed by a function or mapping of some form to produce new, differently labeled objects. You can also modify the axes in-place without creating a new data structure. Here’s a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(\n",
    "    np.arange(12).reshape((3, 4)),\n",
    "    index=['Ohio', 'Colorado', 'New York'],\n",
    "    columns=['one', 'two', 'three', 'four'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like a Series, the axis indexes have a map method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = lambda x: x[:4].upper()\n",
    "data.index.map(transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`Index(['OHIO', 'COLO', 'NEW '], dtype='object')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can assign to index, modifying the DataFrame in-place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.index = data.index.map(transform)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "      one two        three     four\n",
    "OHIO     0    1          2        3\n",
    "COLO     4    5          6        7\n",
    "NEW      8    9         10       11\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to create a transformed version of a dataset without modifying the original, a useful method is rename:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(index=str.title, columns=str.upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "         ONE TWO THREE FOUR\n",
    "Ohio     0   1      2     3\n",
    "Colo     4   5      6     7\n",
    "New      8   9     10    11\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notably, rename can be used in conjunction with a dict-like object providing new values for a subset of the axis labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(\n",
    "    index={'OHIO': 'INDIANA'},\n",
    "    columns={'three': 'peekaboo'},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "            one two peekaboo four\n",
    "INDIANA    0    1         2     3\n",
    "COLO       4    5         6     7\n",
    "NEW        8    9        10    11\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rename saves you from the chore of copying the DataFrame manually and assigning to its index and columns attributes. Should you wish to modify a dataset in-place, pass inplace=True:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(index={'OHIO': 'INDIANA'}, inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "         one two three four\n",
    "INDIANA    0   1     2    3\n",
    "COLO       4     5      6      7\n",
    "NEW        8     9     10     11\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discretization and Binning\n",
    "\n",
    "Continuous data is often discretized or otherwise separated into “bins” for analysis. Suppose you have data about a group of people in a study, and you want to group them into discrete age buckets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages = [20, 22, 25, 27, 21, 23, 37, 31, 61, 45, 41, 32]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s divide these into bins of 18 to 25, 26 to 35, 36 to 60, and finally 61 and older. To do so, you have to use cut, a function in pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [18, 25, 35, 60, 100]\n",
    "cats = pd.cut(ages, bins)\n",
    "cats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "[(18, 25], (18, 25], (18, 25], (25, 35], (18, 25], ..., (25, 35], (60, 100], (35,\n",
    " 60], (35, 60], (25, 35]]\n",
    "Length: 12\n",
    "Categories (4, interval[int64]): [(18, 25] < (25, 35] < (35, 60] < (60, 100]]\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object pandas returns is a special Categorical object. The output you see describes the bins computed by pandas.cut. You can treat it like an array of strings indicating the bin name; internally it contains a categories array specifying the distinct category names along with a labeling for the ages data in the codes attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`array([0, 0, 0, 1, 0, 0, 2, 1, 3, 2, 2, 1], dtype=int8)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats.categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "IntervalIndex([(18, 25], (25, 35], (35, 60], (60, 100]]\n",
    "               closed='right',\n",
    "               dtype='interval[int64]')\n",
    "    `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "(18, 25]     5\n",
    "(35, 60]     3\n",
    "(25, 35]     3\n",
    "(60, 100]    1\n",
    "dtype: int64\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that pd.value_counts(cats) are the bin counts for the result of pandas.cut.\n",
    "\n",
    "Consistent with mathematical notation for intervals, a parenthesis means that the side is open, while the square bracket means it is closed (inclusive). You can change which side is closed by passing right=False:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.cut(ages, [18, 26, 36, 61, 100], right=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "[[18, 26), [18, 26), [18, 26), [26, 36), [18, 26), ..., [26, 36), [61, 100), [36,\n",
    " 61), [36, 61), [26, 36)]\n",
    "Length: 12\n",
    "Categories (4, interval[int64]): [[18, 26) < [26, 36) < [36, 61) < [61, 100)]\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also pass your own bin names by passing a list or array to the labels option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_names = ['Youth', 'YoungAdult', 'MiddleAged', 'Senior']\n",
    "pd.cut(ages, bins, labels=group_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "[Youth, Youth, Youth, YoungAdult, Youth, ..., YoungAdult, Senior, MiddleAged, Mid\n",
    " dleAged, YoungAdult]\n",
    "Length: 12\n",
    "Categories (4, object): [Youth < YoungAdult < MiddleAged < Senior]\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you pass an integer number of bins to cut instead of explicit bin edges, it will compute equal-length bins based on the minimum and maximum values in the data. Consider the case of some uniformly distributed data chopped into fourths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.rand(20)\n",
    "pd.cut(data, 4, precision=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "[(0.34, 0.55], (0.34, 0.55], (0.76, 0.97], (0.76, 0.97], (0.34, 0.55], ..., (0.34\n",
    " , 0.55], (0.34, 0.55], (0.55, 0.76], (0.34, 0.55], (0.12, 0.34]]\n",
    "Length: 20\n",
    "Categories (4, interval[float64]): [(0.12, 0.34] < (0.34, 0.55] < (0.55, 0.76] <\n",
    "    (0.76, 0.97]]\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precision=2 option limits the decimal precision to two digits.\n",
    "\n",
    "A closely related function, qcut, bins the data based on sample quantiles. Depending on the distribution of the data, using cut will not usually result in each bin having the same number of data points. Since qcut uses sample quantiles instead, by definition you will obtain roughly equal-size bins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.randn(1000)              # Normally distributed\n",
    "cats = pd.qcut(data, 4)               # Cut into quartiles\n",
    "cats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "[(-0.0265, 0.62], (0.62, 3.928], (-0.68, -0.0265], (0.62, 3.928], (-0.0265, 0.62]\n",
    " , ..., (-0.68, -0.0265], (-0.68, -0.0265], (-2.95, -0.68], (0.62, 3.928], (-0.68,\n",
    " -0.0265]]\n",
    "Length: 1000\n",
    "Categories (4, interval[float64]): [(-2.95, -0.68] < (-0.68, -0.0265] < (-0.0265,\n",
    "       0.62] < (0.62, 3.928]]\n",
    "    `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "(0.62, 3.928]       250\n",
    "(-0.0265, 0.62]     250\n",
    "(-0.68, -0.0265]    250\n",
    "(-2.95, -0.68]      250\n",
    "dtype: int64\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to cut you can pass your own quantiles (numbers between 0 and 1, inclusive):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.qcut(data, [0, 0.1, 0.5, 0.9, 1.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "[(-0.0265, 1.286], (-0.0265, 1.286], (-1.187, -0.0265], (-0.0265, 1.286], (-0.026\n",
    " 5, 1.286], ..., (-1.187, -0.0265], (-1.187, -0.0265], (-2.95, -1.187], (-0.0265,\n",
    " 1.286], (-1.187, -0.0265]]\n",
    "Length: 1000\n",
    "Categories (4, interval[float64]): [(-2.95, -1.187] < (-1.187, -0.0265] < (-0.026\n",
    "    5, 1.286] < (1.286, 3.928]]\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll return to cut and qcut later in the chapter during our discussion of aggregation and group operations, as these discretization functions are especially useful for quantile and group analysis.\n",
    "\n",
    "# Detecting and Filtering Outliers\n",
    "\n",
    "Filtering or transforming outliers is largely a matter of applying array operations. Consider a DataFrame with some normally distributed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(np.random.randn(1000, 4))\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "                 0            1         2                3\n",
    "count 1000.000000 1000.000000 1000.000000      1000.000000\n",
    "mean      0.049091     0.026112 -0.002544        -0.051827\n",
    "std       0.996947     1.007458  0.995232         0.998311\n",
    "min      -3.645860    -3.184377 -3.745356        -3.428254\n",
    "25%      -0.599807    -0.612162 -0.687373        -0.747478\n",
    "50%       0.047101    -0.013609 -0.022158        -0.088274\n",
    "75%       0.756646     0.695298  0.699046         0.623331\n",
    "max       2.653656     3.525865  2.735527         3.366626\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you wanted to find values in one of the columns exceeding 3 in absolute value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = data[2]\n",
    "col[np.abs(col) > 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "41    -3.399312\n",
    "136   -3.745356\n",
    "Name: 2, dtype: float64\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select all rows having a value exceeding 3 or -3, you can use the any method on a boolean DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[(np.abs(data) > 3).any(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "            0         1         2         3\n",
    "41   0.457246 -0.025907 -3.399312 -0.974657\n",
    "60   1.951312 3.260383 0.963301 1.201206\n",
    "136 0.508391 -0.196713 -3.745356 -1.520113\n",
    "235 -0.242459 -3.056990 1.918403 -0.578828\n",
    "258 0.682841 0.326045 0.425384 -3.428254\n",
    "322 1.179227 -3.184377 1.369891 -1.074833\n",
    "544 -3.548824 1.553205 -2.186301 1.277104\n",
    "635 -0.578093 0.193299 1.397822 3.366626\n",
    "782 -0.207434 3.525865 0.283070 0.544635\n",
    "803 -3.645860 0.255475 -0.549574 -1.907459\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values can be set based on these criteria. Here is code to cap values outside the interval -3 to 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[np.abs(data) > 3] = np.sign(data) * 3\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "                 0            1         2           3\n",
    "count 1000.000000 1000.000000 1000.000000 1000.000000\n",
    "mean      0.050286     0.025567 -0.001399   -0.051765\n",
    "std       0.992920     1.004214  0.991414    0.995761\n",
    "min      -3.000000    -3.000000 -3.000000   -3.000000\n",
    "25%      -0.599807    -0.612162 -0.687373   -0.747478\n",
    "50%       0.047101    -0.013609 -0.022158   -0.088274\n",
    "75%       0.756646     0.695298  0.699046    0.623331\n",
    "max       2.653656     3.000000  2.735527    3.000000\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statement np.sign(data) produces 1 and -1 values based on whether the values in data are positive or negative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sign(data).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "    0    1    2    3\n",
    "0 -1.0 1.0 -1.0 1.0\n",
    "1 1.0 -1.0 1.0 -1.0\n",
    "2 1.0 1.0 1.0 -1.0\n",
    "3 -1.0 -1.0 1.0 -1.0\n",
    "4 -1.0 1.0 -1.0 -1.0\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permutation and Random Sampling\n",
    "\n",
    "Permuting (randomly reordering) a Series or the rows in a DataFrame is easy to do using the numpy.random.permutation function. Calling permutation with the length of the axis you want to permute produces an array of integers indicating the new ordering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.arange(5 * 4).reshape((5, 4)))\n",
    "sampler = np.random.permutation(5)\n",
    "sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`array([3, 1, 4, 2, 0])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That array can then be used in iloc-based indexing or the equivalent take function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "    0   1   2   3\n",
    "0   0   1   2   3\n",
    "1   4   5   6   7\n",
    "2   8   9  10  11\n",
    "3  12  13  14  15\n",
    "4  16  17  18  19\n",
    "    `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.take(sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "    0   1   2   3\n",
    "3  12  13  14  15\n",
    "1   4   5   6   7\n",
    "4  16  17  18  19\n",
    "2   8   9  10  11\n",
    "0   0   1   2   3\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select a random subset without replacement, you can use the sample method on Series and DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "    0   1   2   3\n",
    "3  12  13  14  15\n",
    "4  16  17  18  19\n",
    "2   8   9  10  11\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate a sample with replacement (to allow repeat choices), pass replace=True to sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choices = pd.Series([5, 7, -1, 6, 4])\n",
    "draws = choices.sample(n=10, replace=True)\n",
    "draws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "4    4\n",
    "1    7\n",
    "4    4\n",
    "2   -1\n",
    "0    5\n",
    "3    6\n",
    "1    7\n",
    "4    4\n",
    "0    5\n",
    "4    4\n",
    "dtype: int64\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Indicator/Dummy Variables\n",
    "\n",
    "Another type of transformation for statistical modeling or machine learning applications is converting a categorical variable into a “dummy” or “indicator” matrix. If a column in a DataFrame has k distinct values, you would derive a matrix or DataFrame with k columns containing all 1s and 0s. pandas has a get_dummies function for doing this, though devising one yourself is not difficult. Let’s return to an earlier example DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'key': ['b', 'b', 'a', 'c', 'a', 'b'],\n",
    "    'data1': range(6),\n",
    "})\n",
    "pd.get_dummies(df['key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "  a b c\n",
    "0 0 1 0\n",
    "1 0 1 0\n",
    "2 1 0 0\n",
    "3 0 0 1\n",
    "4 1 0 0\n",
    "5 0 1 0\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, you may want to add a prefix to the columns in the indicator DataFrame, which can then be merged with the other data. get_dummies has a prefix argument for doing this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = pd.get_dummies(df['key'], prefix='key')\n",
    "df_with_dummy = df[['data1']].join(dummies)\n",
    "df_with_dummy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "   data1  key_a  key_b  key_c\n",
    "0      0      0      1      0\n",
    "1      1      0      1      0\n",
    "2      2      1      0      0\n",
    "3      3      0      0      1\n",
    "4      4      1      0      0\n",
    "5      5      0      1      0\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a row in a DataFrame belongs to multiple categories, things are a bit more complicated. Let’s look at the MovieLens 1M dataset, which is investigated in more detail in Chapter 14:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnames = ['movie_id', 'title', 'genres']\n",
    "movies = pd.read_table(\n",
    "    'datasets/movielens/movies.dat',\n",
    "    sep = '::',\n",
    "    header = None,\n",
    "    names = mnames,\n",
    ")\n",
    "movies[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "   movie_id                                 title                       genres\n",
    "0         1                   Toy Story    (1995)  Animation|Children's|Comedy\n",
    "1         2                     Jumanji    (1995) Adventure|Children's|Fantasy\n",
    "2         3            Grumpier Old Men    (1995)               Comedy|Romance\n",
    "3         4           Waiting to Exhale    (1995)                 Comedy|Drama\n",
    "4         5 Father of the Bride Part II    (1995)                       Comedy\n",
    "5         6                        Heat    (1995)        Action|Crime|Thriller\n",
    "6         7                     Sabrina    (1995)               Comedy|Romance\n",
    "7         8                Tom and Huck    (1995)         Adventure|Children's\n",
    "8         9                Sudden Death    (1995)                       Action\n",
    "9        10                   GoldenEye    (1995)    Action|Adventure|Thriller\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding indicator variables for each genre requires a little bit of wrangling. First, we extract the list of unique genres in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_genres = []\n",
    "for x in movies.genres:\n",
    "    all_genres.extend(x.split('|'))\n",
    "genres = pd.unique(all_genres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "array(['Animation', \"Children's\", 'Comedy', 'Adventure', 'Fantasy',\n",
    "       'Romance', 'Drama', 'Action', 'Crime', 'Thriller', 'Horror',\n",
    "       'Sci-Fi', 'Documentary', 'War', 'Musical', 'Mystery', 'Film-Noir',\n",
    "       'Western'], dtype=object)\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to construct the indicator DataFrame is to start with a DataFrame of all zeros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_matrix = np.zeros((len(movies), len(genres)))\n",
    "dummies = pd.DataFrame(zero_matrix, columns=genres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, iterate through each movie and set entries in each row of dummies to 1. To do this, we use the dummies.columns to compute the column indices for each genre:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = movies.genres[0]\n",
    "gen.split('|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`['Animation', \"Children's\", 'Comedy']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies.columns.get_indexer(gen.split('|'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`array([0, 1, 2])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can use .iloc to set values based on these indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, gen in enumerate(movies.genres):\n",
    "    indices = dummies.columns.get_indexer(gen.split('|'))\n",
    "    dummies.iloc[i, indices] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, as before, you can combine this with movies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_windic = movies.join(dummies.add_prefix('Genre_'))\n",
    "movies_windic.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "movie_id                                       1\n",
    "title                           Toy Story (1995)\n",
    "genres               Animation|Children's|Comedy\n",
    "Genre_Animation                                1\n",
    "Genre_Children's                               1\n",
    "Genre_Comedy                                   1\n",
    "Genre_Adventure                                0\n",
    "Genre_Fantasy                                  0\n",
    "Genre_Romance                                  0\n",
    "Genre_Drama                                    0\n",
    "                                ...\n",
    "Genre_Crime                                    0\n",
    "Genre_Thriller                                 0\n",
    "Genre_Horror                                   0\n",
    "Genre_Sci-Fi                                   0\n",
    "Genre_Documentary                              0\n",
    "Genre_War                                      0\n",
    "Genre_Musical                                  0\n",
    "Genre_Mystery                                  0\n",
    "Genre_Film-Noir                                0\n",
    "Genre_Western                                  0\n",
    "Name: 0, Length: 21, dtype: object\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_Note_**<br>For much larger data, this method of constructing indicator variables with multiple membership is not especially speedy. It would be better to write a lower-level function that writes directly to a NumPy array, and then wrap the result in a DataFrame.\n",
    "\n",
    "A useful recipe for statistical applications is to combine get_dummies with a discretization function like cut:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12345)\n",
    "values = np.random.rand(10)\n",
    "values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "array([ 0.9296, 0.3164,    0.1839, 0.2046,    0.5677,    0.5955,     0.9645,\n",
    "        0.6532, 0.7489,    0.6536])\n",
    "    `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0, 0.2, 0.4, 0.6, 0.8, 1]\n",
    "pd.get_dummies(pd.cut(values, bins))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "   (0.0, 0.2] (0.2, 0.4] (0.4, 0.6] (0.6, 0.8]          (0.8, 1.0]\n",
    "0           0           0           0          0                 1\n",
    "1           0           1           0          0                 0\n",
    "2           1           0           0          0                 0\n",
    "3           0           1           0          0                 0\n",
    "4           0           0           1          0                 0\n",
    "5           0           0           1          0                 0\n",
    "6           0           0           0          0                 1\n",
    "7           0           0           0          1                 0\n",
    "8           0           0           0          1                 0\n",
    "9           0           0           0          1                 0\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the random seed with numpy.random.seed to make the example deterministic. We will look again at pandas.get_dummies later in the book.\n",
    "\n",
    "# String Manipulation\n",
    "\n",
    "Python has long been a popular raw data manipulation language in part due to its ease of use for string and text processing. Most text operations are made simple with the string object’s built-in methods. For more complex pattern matching and text manipulations, regular expressions may be needed. pandas adds to the mix by enabling you to apply string and regular expressions concisely on whole arrays of data, additionally handling the annoyance of missing data.\n",
    "\n",
    "# String Object Methods\n",
    "\n",
    "In many string munging and scripting applications, built-in string methods are sufficient. As an example, a comma-separated string can be broken into pieces with split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = 'a,b,   guido'\n",
    "val.split(',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`['a', 'b', ' guido']`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split is often combined with strip to trim whitespace (including line breaks):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pieces = [x.strip() for x in val.split(',')]\n",
    "pieces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`['a', 'b', 'guido']`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These substrings could be concatenated together with a two-colon delimiter using addition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first, second, third = pieces\n",
    "first + '::' + second + '::' + third"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`'a::b::guido'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this isn’t a practical generic method. A faster and more Pythonic way is to pass a list or tuple to the join method on the string '::':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'::'.join(pieces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`'a::b::guido'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other methods are concerned with locating substrings. Using Python’s in keyword is the best way to detect a substring, though index and find can also be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'guido' in val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val.index(',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val.find(':')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`-1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the difference between find and index is that index raises an exception if the string isn’t found (versus returning -1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val.index(':')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "ValueError                                Traceback (most recent call last)\n",
    "<ipython-input-144-280f8b2856ce> in <module>()\n",
    "\\-\\-\\-\\-> 1 val.index(':')\n",
    "ValueError: substring not found\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relatedly, count returns the number of occurrences of a particular substring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val.count(',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "replace will substitute occurrences of one pattern for another. It is commonly used to delete patterns, too, by passing an empty string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val.replace(',', '::')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`'a::b:: guido'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val.replace(',', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`'ab guido'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See Table 7-3 for a listing of some of Python’s string methods.\n",
    "\n",
    "Regular expressions can also be used with many of these operations, as you’ll see.\n",
    "\n",
    "_Table 7-3. Python built-in string methods_\n",
    "\n",
    "| Argument | Description |\n",
    "|----------|--------------------------------------------|\n",
    "| `count`      | Return the number of non-overlapping occurrences of substring in the string. |\n",
    "| `endswith`   | Returns True if string ends with suffix. |\n",
    "| `startswith` | Returns True if string starts with prefix. |\n",
    "| `join`       | Use string as delimiter for concatenating a sequence of other strings. |\n",
    "| `index`      | Return position of first character in substring if found in the string; raises ValueError if not found. |\n",
    "| `find`       | Return position of first character of first occurrence of substring in the string; like index, but returns –1 if not found. |\n",
    "| `rfind`      | Return position of first character of last occurrence of substring in the string; returns –1 if not found. |\n",
    "| `replace`    | Replace occurrences of string with another string. |\n",
    "| `strip`, `rstrip`, `lstrip` | Trim whitespace, including newlines; equivalent to x.strip() (and rstrip, lstrip, respectively) for each element. |\n",
    "| `split`      | Break string into list of substrings using passed delimiter. |\n",
    "| `lower`      | Convert alphabet characters to lowercase. |\n",
    "| `upper`      | Convert alphabet characters to uppercase. |\n",
    "| `casefold`   | Convert characters to lowercase, and convert any region-specific variable character combinations to a common comparable form. |\n",
    "| `ljust`, `rjust` | Left justify or right justify, respectively; pad opposite side of string with spaces (or some other fill character) to return a string with a minimum width. |\n",
    "\n",
    "# Regular Expressions\n",
    "\n",
    "Regular expressions provide a flexible way to search or match (often more complex) string patterns in text. A single expression, commonly called a regex, is a string formed according to the regular expression language. Python’s built-in re module is responsible for applying regular expressions to strings; I’ll give a number of examples of its use here.\n",
    "\n",
    "> **_Note_**<br>The art of writing regular expressions could be a chapter of its own and thus is outside the book’s scope. There are many excellent tutorials and references available on the internet and in other books.\n",
    "\n",
    "The re module functions fall into three categories: pattern matching, substitution, and splitting. Naturally these are all related; a regex describes a pattern to locate in the text, which can then be used for many purposes. Let’s look at a simple example: suppose we wanted to split a string with a variable number of whitespace characters (tabs, spaces, and newlines). The regex describing one or more whitespace characters is \\s+:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "text = \"foo             bar\\t baz    \\tqux\"\n",
    "re.split('\\s+', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`['foo', 'bar', 'baz', 'qux']`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you call re.split('\\s+', text), the regular expression is first compiled, and then its split method is called on the passed text. You can compile the regex yourself with re.compile, forming a reusable regex object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = re.compile('\\s+')\n",
    "regex.split(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`['foo', 'bar', 'baz', 'qux']`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If, instead, you wanted to get a list of all patterns matching the regex, you can use the findall method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex.findall(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`['    ', '\\t ', ' \\t']`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_Note_**<br>To avoid unwanted escaping with \\\\ in a regular expression, use raw string literals like r'C:\\\\x' instead of the equivalent 'C:\\\\\\\\x'.\n",
    "\n",
    "Creating a regex object with re.compile is highly recommended if you intend to apply the same expression to many strings; doing so will save CPU cycles. match and search are closely related to findall. While findall returns all matches in a string, search returns only the first match. More rigidly, match only matches at the beginning of the string. As a less trivial example, let’s consider a block of text and a regular expression capable of identifying most email addresses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Dave dave@google.com\n",
    "Steve steve@gmail.com\n",
    "Rob rob@gmail.com\n",
    "Ryan ryan@yahoo.com\n",
    "\"\"\"\n",
    "pattern = r'[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]{2,4}'\n",
    "# re.IGNORECASE makes the regex case-insensitive\n",
    "regex = re.compile(pattern, flags=re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using findall on the text produces a list of the email addresses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex.findall(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "['dave@google.com',\n",
    " 'steve@gmail.com',\n",
    " 'rob@gmail.com',\n",
    " 'ryan@yahoo.com']\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "search returns a special match object for the first email address in the text. For the preceding regex, the match object can only tell us the start and end position of the pattern in the string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = regex.search(text)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`<_sre.SRE_Match object; span=(5, 20), match='dave@google.com'>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text[m.start():m.end()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`'dave@google.com'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regex.match returns None, as it only will match if the pattern occurs at the start of the string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(regex.match(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`None`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relatedly, sub will return a new string with occurrences of the pattern replaced by the a new string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(regex.sub('REDACTED', text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "Dave REDACTED\n",
    "Steve REDACTED\n",
    "Rob REDACTED\n",
    "Ryan REDACTED\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you wanted to find email addresses and simultaneously segment each address into its three components: username, domain name, and domain suffix. To do this, put parentheses around the parts of the pattern to segment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'([A-Z0-9._%+-]+)@([A-Z0-9.-]+)\\.([A-Z]{2,4})'\n",
    "regex = re.compile(pattern, flags=re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A match object produced by this modified regex returns a tuple of the pattern components with its groups method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = regex.match('wesm@bright.net')\n",
    "m.groups()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`('wesm', 'bright', 'net')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "findall returns a list of tuples when the pattern has groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex.findall(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "[('dave', 'google', 'com'),\n",
    " ('steve', 'gmail', 'com'),\n",
    " ('rob', 'gmail', 'com'),\n",
    " ('ryan', 'yahoo', 'com')]\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sub also has access to groups in each match using special symbols like \\1 and \\2. The symbol \\1 corresponds to the first matched group, \\2 corresponds to the second, and so forth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(regex.sub(r'Username: \\1, Domain: \\2, Suffix: \\3', text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "Dave Username: dave, Domain: google, Suffix: com\n",
    "Steve Username: steve, Domain: gmail, Suffix: com\n",
    "Rob Username: rob, Domain: gmail, Suffix: com\n",
    "Ryan Username: ryan, Domain: yahoo, Suffix: com\n",
    "        `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is much more to regular expressions in Python, most of which is outside the book’s scope. Table 7-4 provides a brief summary.\n",
    "\n",
    "_Table 7-4. Regular expression methods_\n",
    "\n",
    "| Argument | Description |\n",
    "|----------|-----------------------------------------------|\n",
    "| `findall`   | Return all non-overlapping matching patterns in a string as a list |\n",
    "| `finditer`  | Like findall, but returns an iterator |\n",
    "| `match`     | Match pattern at start of string and optionally segment pattern components into groups; if the pattern matches, returns a match object, and otherwise None |\n",
    "| `search`    | Scan string for match to pattern; returning a match object if so; unlike match, the match can be anywhere in the string as opposed to only at the beginning |\n",
    "| `split`     | Break string into pieces at each occurrence of pattern |\n",
    "| `sub`, `subn` | Replace all (sub) or first n occurrences (subn) of pattern in string with replacement expression; use symbols \\1, \\2, ... to refer to match group elements in the replacement string |\n",
    "\n",
    "# Vectorized String Functions in pandas\n",
    "\n",
    "Cleaning up a messy dataset for analysis often requires a lot of string munging and regularization. To complicate matters, a column containing strings will sometimes have missing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'Dave': 'dave@google.com',\n",
    "    'Steve': 'steve@gmail.com',\n",
    "    'Rob': 'rob@gmail.com',\n",
    "    'Wes': np.nan,\n",
    "}\n",
    "data = pd.Series(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "Dave      dave@google.com\n",
    "Rob         rob@gmail.com\n",
    "Steve     steve@gmail.com\n",
    "Wes                   NaN\n",
    "dtype: object\n",
    "    `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "Dave      False\n",
    "Rob       False\n",
    "Steve     False\n",
    "Wes        True\n",
    "dtype: bool\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can apply string and regular expression methods can be applied (passing a lambda or other function) to each value using data.map, but it will fail on the NA (null) values. To cope with this, Series has array-oriented methods for string operations that skip NA values. These are accessed through Series’s str attribute; for example, we could check whether each email address has 'gmail' in it with str.contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.str.contains('gmail')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "Dave      False\n",
    "Rob        True\n",
    "Steve      True\n",
    "Wes         NaN\n",
    "dtype: object\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular expressions can be used, too, along with any re options like IGNORECASE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`'([A-Z0-9._%+-]+)@([A-Z0-9.-]+)\\\\.([A-Z]{2,4})'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.str.findall(pattern, flags=re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "Dave      [(dave, google, com)]\n",
    "Rob         [(rob, gmail, com)]\n",
    "Steve     [(steve, gmail, com)]\n",
    "Wes                         NaN\n",
    "dtype: object\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple of ways to do vectorized element retrieval. Either use str.get or index into the str attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = data.str.match(pattern, flags=re.IGNORECASE)\n",
    "matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "Dave      True\n",
    "Rob       True\n",
    "Steve     True\n",
    "Wes        NaN\n",
    "dtype: object\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access elements in the embedded lists, we can pass an index to either of these functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches.str.get(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "Dave    NaN\n",
    "Rob     NaN\n",
    "Steve   NaN\n",
    "Wes     NaN\n",
    "dtype: float64\n",
    "    `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches.str[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "Dave    NaN\n",
    "Rob     NaN\n",
    "Steve   NaN\n",
    "Wes     NaN\n",
    "dtype: float64\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can similarly slice strings using this syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.str[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:<br>`\n",
    "Dave      dave@\n",
    "Rob       rob@g\n",
    "Steve     steve\n",
    "Wes         NaN\n",
    "dtype: object\n",
    "    `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See Table 7-5 for more pandas string methods.\n",
    "\n",
    "_Table 7-5. Partial listing of vectorized string methods_\n",
    "\n",
    "| Method | Description |\n",
    "|------------|-------------------------------------------------|\n",
    "| `cat`          | Concatenate strings element-wise with optional delimiter |\n",
    "| `contains`     | Return boolean array if each string contains pattern/regex |\n",
    "| `count`        | Count occurrences of pattern |\n",
    "| `extract`      | Use a regular expression with groups to extract one or more strings from a Series of strings; the result will be a DataFrame with one column per group |\n",
    "| `endswith`     | Equivalent to x.endswith(pattern) for each element |\n",
    "| `startswith`   | Equivalent to x.startswith(pattern) for each element |\n",
    "| `findall`      | Compute list of all occurrences of pattern/regex for each string |\n",
    "| `get`          | Index into each element (retrieve i-th element) |\n",
    "| `isalnum`      | Equivalent to built-in str.alnum |\n",
    "| `isalpha`      | Equivalent to built-in str.isalpha |\n",
    "| `isdecimal`    | Equivalent to built-in str.isdecimal |\n",
    "| `isdigit`      | Equivalent to built-in str.isdigit |\n",
    "| `islower`      | Equivalent to built-in str.islower |\n",
    "| `isnumeric`    | Equivalent to built-in str.isnumeric |\n",
    "| `isupper`      | Equivalent to built-in str.isupper |\n",
    "| `join`         | Join strings in each element of the Series with passed separator |\n",
    "| `len`          | Compute length of each string |\n",
    "| `lower`, `upper` | Convert cases; equivalent to x.lower() or x.upper() for each element |\n",
    "| `match`        | Use re.match with the passed regular expression on each element, returning matched groups as list |\n",
    "| `pad`          | Add whitespace to left, right, or both sides of strings |\n",
    "| `center`       | Equivalent to pad(side='both') |\n",
    "| `repeat`       | Duplicate values (e.g., s.str.repeat(3) is equivalent to x * 3 for each string) |\n",
    "| `replace`      | Replace occurrences of pattern/regex with some other string |\n",
    "| `slice`        | Slice each string in the Series |\n",
    "| `split`        | Split strings on delimiter or regular expression |\n",
    "| `strip`        | Trim whitespace from both sides, including newlines |\n",
    "| `rstrip`       | Trim whitespace on right side |\n",
    "| `lstrip`       | Trim whitespace on left side |\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "Effective data preparation can significantly improve productive by enabling you to spend more time analyzing data and less time getting it ready for analysis. We have explored a number of tools in this chapter, but the coverage here is by no means comprehensive. In the next chapter, we will explore pandas’s joining and grouping functionality."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
