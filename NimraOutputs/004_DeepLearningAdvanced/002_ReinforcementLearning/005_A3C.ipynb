{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A3C\n",
    "\n",
    "In this section you will learn how to implement A3C, the asynchronous reinforcement learning algorithm you saw earlier in the chapter. A3C is a significantly more complex training algorithm than those you have seen previously. The algorithm requires running gradient descent in multiple threads, interspersed with game rollout code, and updating learned weights asynchronously. As a result of this extra complexity, we will define the A3C algorithm in an object-oriented fashion. Let’s start by defining an A3C object.\n",
    "\n",
    "The A3C class implements the A3C algorithm (Example 8-16). A few extra bells and whistles are added onto the basic algorithm to encourage learning, notably an entropy term and support for generalized advantage estimation. We won’t cover all of these details, but encourage you to follow references into the research literature (listed in the documentation) to understand more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 8-16. Define the A3C class encapsulating the asynchronous A3C training algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A3C(object):\n",
    "  \"\"\"\n",
    "  Implements the Asynchronous Advantage Actor-Critic (A3C) algorithm.\n",
    "  The algorithm is described in Mnih et al, \"Asynchronous Methods for Deep\n",
    "  Reinforcement Learning\" (https://arxiv.org/abs/1602.01783). This class\n",
    "  requires the policy to output two quantities: a vector giving the probability\n",
    "  of taking each action, and an estimate of the value function for the current\n",
    "  state. It optimizes both outputs at once using a loss that is the sum of three\n",
    "  terms:\n",
    "  1. The policy loss, which seeks to maximize the discounted reward for each action.\n",
    "  2. The value loss, which tries to make the value estimate match the actual\n",
    "     discounted reward that was attained at each step.\n",
    "  3. An entropy term to encourage exploration.\n",
    "  This class only supports environments with discrete action spaces, not\n",
    "  continuous ones. The \"action\" argument passed to the environment is an\n",
    "  integer, giving the index of the action to perform.\n",
    "  This class supports Generalized Advantage Estimation as described in Schulman\n",
    "  et al., \"High-Dimensional Continuous Control Using Generalized Advantage\n",
    "  Estimation\" (https://arxiv.org/abs/1506.02438). This is a method of trading\n",
    "  off bias and variance in the advantage estimate, which can sometimes improve\n",
    "  the rate of convergence. Use the advantage_lambda parameter to adjust the\n",
    "  tradeoff.\n",
    "  \"\"\"\n",
    "  self._env = env\n",
    "  self.max_rollout_length = max_rollout_length\n",
    "  self.discount_factor = discount_factor\n",
    "  self.advantage_lambda = advantage_lambda\n",
    "  self.value_weight = value_weight\n",
    "  self.entropy_weight = entropy_weight\n",
    "  self._optimizer = None\n",
    "  (self._graph, self._features, self._rewards, self._actions,\n",
    "   self._action_prob, self._value, self._advantages) = self.build_graph(\n",
    "       None, \"global\", model_dir)\n",
    "  with self._graph._get_tf(\"Graph\").as_default():\n",
    "    self._session = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heart of the A3C class lies in the build_graph() method (Example 8-17), which constructs a TensorGraph instance (underneath which lies a TensorFlow computation graph) encoding the policy learned by the model. Notice how succinct this definition is compared with others you have seen previously! There are many advantages to using object orientation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 8-17. This method builds the computation graph for the A3C algorithm. Note that the policy network is defined here using the Layer abstractions you saw previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(self, tf_graph, scope, model_dir):\n",
    "  \"\"\"Construct a TensorGraph containing the policy and loss calculations.\"\"\"\n",
    "  state_shape = self._env.state_shape\n",
    "  features = []\n",
    "  for s in state_shape:\n",
    "    features.append(Input(shape=[None] + list(s), dtype=tf.float32))\n",
    "  d1 = Flatten(in_layers=features)\n",
    "  d2 = Dense(\n",
    "      in_layers=[d1],\n",
    "      activation_fn=tf.nn.relu,\n",
    "      normalizer_fn=tf.nn.l2_normalize,\n",
    "      normalizer_params={\"dim\": 1},\n",
    "      out_channels=64)\n",
    "  d3 = Dense(\n",
    "      in_layers=[d2],\n",
    "      activation_fn=tf.nn.relu,\n",
    "      normalizer_fn=tf.nn.l2_normalize,\n",
    "      normalizer_params={\"dim\": 1},\n",
    "      out_channels=32)\n",
    "  d4 = Dense(\n",
    "      in_layers=[d3],\n",
    "      activation_fn=tf.nn.relu,\n",
    "      normalizer_fn=tf.nn.l2_normalize,\n",
    "      normalizer_params={\"dim\": 1},\n",
    "      out_channels=16)\n",
    "  d4 = BatchNorm(in_layers=[d4])\n",
    "  d5 = Dense(in_layers=[d4], activation_fn=None, out_channels=9)\n",
    "  value = Dense(in_layers=[d4], activation_fn=None, out_channels=1)\n",
    "  value = Squeeze(squeeze_dims=1, in_layers=[value])\n",
    "  action_prob = SoftMax(in_layers=[d5])\n",
    "  rewards = Input(shape=(None,))\n",
    "  advantages = Input(shape=(None,))\n",
    "  actions = Input(shape=(None, self._env.n_actions))\n",
    "  loss = A3CLoss(\n",
    "      self.value_weight,\n",
    "      self.entropy_weight,\n",
    "      in_layers=[rewards, actions, action_prob, value, advantages])\n",
    "  graph = TensorGraph(\n",
    "      batch_size=self.max_rollout_length,\n",
    "      graph=tf_graph,\n",
    "      model_dir=model_dir)\n",
    "  for f in features:\n",
    "    graph._add_layer(f)\n",
    "  graph.add_output(action_prob)\n",
    "  graph.add_output(value)\n",
    "  graph.set_loss(loss)\n",
    "  graph.set_optimizer(self._optimizer)\n",
    "  with graph._get_tf(\"Graph\").as_default():\n",
    "    with tf.variable_scope(scope):\n",
    "      graph.build()\n",
    "  return graph, features, rewards, actions, action_prob, value, advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There’s a lot of code in this example. Let’s break it down into multiple examples and discuss more carefully. Example 8-18 takes the array encoding of the TicTacToeEnvironment and feeds it into the Input instances for the graph directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 8-18. This snippet from the build_graph() method feeds in the array encoding of TicTacToeEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_shape = self._env.state_shape\n",
    "features = []\n",
    "for s in state_shape:\n",
    "  features.append(Input(shape=[None] + list(s), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 8-19 shows the code used to construct inputs for rewards from the environment, advantages observed, and actions taken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 8-19. This snippet from the build_graph() method defines Input objects for rewards, advantages, and actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = Input(shape=(None,))\n",
    "advantages = Input(shape=(None,))\n",
    "actions = Input(shape=(None, self._env.n_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The policy network is responsible for learning the policy. In Example 8-20, the input board state is first flattened into an input feature vector. A series of fully connected (or Dense) transformations are applied to the flattened board. At the very end, a Softmax layer is used to predict action probabilities from d5 (note that out_channels is set to 9, one for each possible move on the tic-tac-toe board)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 8-20. This snippet from the build_graph() method defines the policy network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = Flatten(in_layers=features)\n",
    "d2 = Dense(\n",
    "    in_layers=[d1],\n",
    "    activation_fn=tf.nn.relu,\n",
    "    normalizer_fn=tf.nn.l2_normalize,\n",
    "    normalizer_params={\"dim\": 1},\n",
    "    out_channels=64)\n",
    "d3 = Dense(\n",
    "    in_layers=[d2],\n",
    "    activation_fn=tf.nn.relu,\n",
    "    normalizer_fn=tf.nn.l2_normalize,\n",
    "    normalizer_params={\"dim\": 1},\n",
    "    out_channels=32)\n",
    "d4 = Dense(\n",
    "    in_layers=[d3],\n",
    "    activation_fn=tf.nn.relu,\n",
    "    normalizer_fn=tf.nn.l2_normalize,\n",
    "    normalizer_params={\"dim\": 1},\n",
    "    out_channels=16)\n",
    "d4 = BatchNorm(in_layers=[d4])\n",
    "d5 = Dense(in_layers=[d4], activation_fn=None, out_channels=9)\n",
    "value = Dense(in_layers=[d4], activation_fn=None, out_channels=1)\n",
    "value = Squeeze(squeeze_dims=1, in_layers=[value])\n",
    "action_prob = SoftMax(in_layers=[d5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_Is Feature Engineering Dead?_**<br>In this section, we feed the raw tic-tac-toe game board into TensorFlow for training the policy. However, it’s important to note that for more complex games than tic-tac-toe, this may not yield satisfactory results. One of the lesser known facts about AlphaGo is that DeepMind performs sophisticated feature engineering to extract “interesting” patterns of Go pieces upon the board to make AlphaGo’s learning easier. (This fact is tucked away into the supplemental information of DeepMind’s paper.)<br>The fact remains that reinforcement learning (and deep learning methods broadly) often still need human-guided feature engineering to extract meaningful information before learning algorithms can learn effective policies and models. It’s likely that as more computational power becomes available through hardware advances, this need for feature engineering will be reduced, but for the near term, plan on manually extracting information about your systems as needed for performance.\n",
    "\n",
    "# The A3C Loss Function\n",
    "\n",
    "We now have the object-oriented machinery set in place to define a loss for the A3C policy network. This loss function will itself be implemented as a Layer object (it’s a convenient abstraction that all parts of the deep architecture are simply layers). The A3CLoss object implements a mathematical loss consisting of the sum of three terms: a policy_loss, a value_loss, and an entropy term for exploration. See Example 8-21."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 8-21. This Layer implements the loss function for A3C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A3CLoss(Layer):\n",
    "  \"\"\"This layer computes the loss function for A3C.\"\"\"\n",
    "  def __init__(self, value_weight, entropy_weight, **kwargs):\n",
    "    super(A3CLoss, self).__init__(**kwargs)\n",
    "    self.value_weight = value_weight\n",
    "    self.entropy_weight = entropy_weight\n",
    "  def create_tensor(self, **kwargs):\n",
    "    reward, action, prob, value, advantage = [\n",
    "        layer.out_tensor for layer in self.in_layers\n",
    "    ]\n",
    "    prob = prob + np.finfo(np.float32).eps\n",
    "    log_prob = tf.log(prob)\n",
    "    policy_loss = -tf.reduce_mean(\n",
    "        advantage * tf.reduce_sum(action * log_prob, axis=1))\n",
    "    value_loss = tf.reduce_mean(tf.square(reward - value))\n",
    "    entropy = -tf.reduce_mean(tf.reduce_sum(prob * log_prob, axis=1))\n",
    "    self.out_tensor = policy_loss + self.value_weight * value_loss\n",
    "    - self.entropy_weight * entropy\n",
    "    return self.out_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of pieces to this definition, so let’s pull out bits of code and inspect. The A3CLoss layer takes in reward, action, prob, value, advantage layers as inputs. For mathematical stability, we convert probabilities to log probabilities (this is numerically much more stable). See Example 8-22."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 8-22. This snippet from A3CLoss takes reward, action, prob, value, advantage as input layers and computes a log probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward, action, prob, value, advantage = [\n",
    "    layer.out_tensor for layer in self.in_layers\n",
    "]\n",
    "prob = prob + np.finfo(np.float32).eps\n",
    "log_prob = tf.log(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The policy loss computes the sum of all advantages observed, weighted by the log-probability of the action taken. (Recall that the advantage is the difference in reward resulting from taking the given action as opposed to the expected reward from the raw policy for that state). The intuition here is that the policy_loss provides a signal on which actions were fruitful and which were not (Example 8-23)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 8-23. This snippet from A3CLoss defines the policy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_loss = -tf.reduce_mean(\n",
    "    advantage * tf.reduce_sum(action * log_prob, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value loss computes the difference between our estimate of V (reward) and the actual value of V observed (value). Note the use of the $L^2$ loss here (Example 8-24)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 8-24. This snippet from A3CLoss defines the value loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_loss = tf.reduce_mean(tf.square(reward - value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entropy term is an addition that encourages the policy to explore further by adding some noise. This term is effectively a form of regularization for A3C networks. The final loss computed by A3CLoss is a linear combination of these component losses. See Example 8-25."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 8-25. This snippet from A3CLoss defines an entropy term added to the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy = -tf.reduce_mean(tf.reduce_sum(prob * log_prob, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Workers\n",
    "\n",
    "Thus far, you’ve seen how the policy network is constructed, but you haven’t yet seen how the asynchronous training procedure is implemented. Conceptually, asynchronous training consists of individual workers running gradient descent on locally simulated game rollouts and contributing learned knowledge back to a global set of weights periodically. Continuing our object-oriented design, let’s introduce the Worker class.\n",
    "\n",
    "Each Worker instance holds a copy of the model that’s trained asynchronously on a separate thread (Example 8-26). Note that a3c.build_graph() is used to construct a local copy of the TensorFlow computation graph for the thread in question. Take special note of local_vars and global_vars here. We need to make sure to train only the variables associated with this worker’s copy of the policy and not with the global copy of the variables (which is used to share information across worker threads). As a result gradients uses tf.gradients to take gradients of the loss with respect to only local_vars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 8-26. The Worker class implements the computation performed by each thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Worker(object):\n",
    "  \"\"\"A Worker object is created for each training thread.\"\"\"\n",
    "  def __init__(self, a3c, index):\n",
    "    self.a3c = a3c\n",
    "    self.index = index\n",
    "    self.scope = \"worker%d\" % index\n",
    "    self.env = copy.deepcopy(a3c._env)\n",
    "    self.env.reset()\n",
    "    (self.graph, self.features, self.rewards, self.actions, self.action_prob,\n",
    "     self.value, self.advantages) = a3c.build_graph(\n",
    "        a3c._graph._get_tf(\"Graph\"), self.scope, None)\n",
    "    with a3c._graph._get_tf(\"Graph\").as_default():\n",
    "      local_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                                     self.scope)\n",
    "      global_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                                      \"global\")\n",
    "      gradients = tf.gradients(self.graph.loss.out_tensor, local_vars)\n",
    "      grads_and_vars = list(zip(gradients, global_vars))\n",
    "      self.train_op = a3c._graph._get_tf(\"Optimizer\").apply_gradients(\n",
    "          grads_and_vars)\n",
    "      self.update_local_variables = tf.group(\n",
    "          * [tf.assign(v1, v2) for v1, v2 in zip(local_vars, global_vars)])\n",
    "      self.global_step = self.graph.get_global_step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Worker rollouts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each Worker is responsible for simulating game rollouts locally. The create_rollout() method uses session.run to fetch action probabilities from the TensorFlow graph (Example 8-27). It then samples an action from this policy using np.random.choice, weighted by the per-class probabilities. The reward for the action taken is computed from TicTacToeEnvironment via a call to self.env.step(action)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 8-27. The create_rollout method simulates a game rollout locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rollout(self):\n",
    "  \"\"\"Generate a rollout.\"\"\"\n",
    "  n_actions = self.env.n_actions\n",
    "  session = self.a3c._session\n",
    "  states = []\n",
    "  actions = []\n",
    "  rewards = []\n",
    "  values = []\n",
    "  # Generate the rollout.\n",
    "  for i in range(self.a3c.max_rollout_length):\n",
    "    if self.env.terminated:\n",
    "        break\n",
    "      state = self.env.state\n",
    "      states.append(state)\n",
    "      feed_dict = self.create_feed_dict(state)\n",
    "      results = session.run(\n",
    "          [self.action_prob.out_tensor, self.value.out_tensor],\n",
    "          feed_dict=feed_dict)\n",
    "      probabilities, value = results[:2]\n",
    "      action = np.random.choice(np.arange(n_actions), p=probabilities[0])\n",
    "      actions.append(action)\n",
    "      values.append(float(value))\n",
    "      rewards.append(self.env.step(action))\n",
    "  # Compute an estimate of the reward for the rest of the episode.\n",
    "  if not self.env.terminated:\n",
    "    feed_dict = self.create_feed_dict(self.env.state)\n",
    "    final_value = self.a3c.discount_factor * float(\n",
    "        session.run(self.value.out_tensor, feed_dict))\n",
    "  else:\n",
    "    final_value = 0.0\n",
    "  values.append(final_value)\n",
    "  if self.env.terminated:\n",
    "    self.env.reset()\n",
    "  return states, actions, np.array(rewards), np.array(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process_rollouts() method does preprocessing needed to compute discounted rewards, values, actions, and advantages (Example 8-28)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 8-28. The process_rollout method computes rewards, values, actions, and advantages and then takes a gradient descent step against the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_rollout(self, states, actions, rewards, values, step_count):\n",
    "  \"\"\"Train the network based on a rollout.\"\"\"\n",
    "  # Compute the discounted rewards and advantages.\n",
    "  if len(states) == 0:\n",
    "    # Rollout creation sometimes fails in multithreaded environment.\n",
    "    # Don't process if malformed\n",
    "    print(\"Rollout creation failed. Skipping\")\n",
    "    return\n",
    "  discounted_rewards = rewards.copy()\n",
    "  discounted_rewards[-1] += values[-1]\n",
    "  advantages = rewards - values[:-1] + self.a3c.discount_factor * np.array(\n",
    "      values[1:])\n",
    "  for j in range(len(rewards) - 1, 0, -1):\n",
    "    discounted_rewards[j-1] += self.a3c.discount_factor * discounted_rewards[j]\n",
    "    advantages[j-1] += (\n",
    "        self.a3c.discount_factor * self.a3c.advantage_lambda * advantages[j])\n",
    "   # Convert the actions to one-hot.\n",
    "  n_actions = self.env.n_actions\n",
    "  actions_matrix = []\n",
    "  for action in actions:\n",
    "    a = np.zeros(n_actions)\n",
    "    a[action] = 1.0\n",
    "    actions_matrix.append(a)\n",
    "  # Rearrange the states into the proper set of arrays.\n",
    "  state_arrays = [[] for i in range(len(self.features))]\n",
    "  for state in states:\n",
    "    for j in range(len(state)):\n",
    "      state_arrays[j].append(state[j])\n",
    "  # Build the feed dict and apply gradients.\n",
    "  feed_dict = {}\n",
    "  for f, s in zip(self.features, state_arrays):\n",
    "    feed_dict[f.out_tensor] = s\n",
    "  feed_dict[self.rewards.out_tensor] = discounted_rewards\n",
    "  feed_dict[self.actions.out_tensor] = actions_matrix\n",
    "  feed_dict[self.advantages.out_tensor] = advantages\n",
    "  feed_dict[self.global_step] = step_count\n",
    "  self.a3c._session.run(self.train_op, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Worker.run() method performs the training step for the Worker, relying on process_rollouts() to issue the actual call to self.a3c._session.run() under the hood (Example 8-29)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 8-29. The run() method is the top level invocation for Worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(self, step_count, total_steps):\n",
    "  with self.graph._get_tf(\"Graph\").as_default():\n",
    "    while step_count[0] < total_steps:\n",
    "      self.a3c._session.run(self.update_local_variables)\n",
    "      states, actions, rewards, values = self.create_rollout()\n",
    "      self.process_rollout(states, actions, rewards, values, step_count[0])\n",
    "      step_count[0] += len(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Policy\n",
    "\n",
    "The A3C.fit() method brings together all the disparate pieces introduced to train the model. The fit() method takes the responsibility for spawning Worker threads using the Python threading library. Since each Worker takes responsibility for training itself, the fit() method simply is responsible for periodically checkpointing the trained model to disk. See Example 8-30."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 8-30. The fit() method brings everything together and runs the A3C training algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self,\n",
    "        total_steps,\n",
    "        max_checkpoints_to_keep=5,\n",
    "        checkpoint_interval=600,\n",
    "        restore=False):\n",
    "  \"\"\"Train the policy.\n",
    "  Parameters\n",
    "  ----------\n",
    "  total_steps: int\n",
    "    the total number of time steps to perform on the environment, across all\n",
    "    rollouts on all threads\n",
    "  max_checkpoints_to_keep: int\n",
    "    the maximum number of checkpoint files to keep. When this number is\n",
    "    reached, older files are deleted.\n",
    "  checkpoint_interval: float\n",
    "    the time interval at which to save checkpoints, measured in seconds\n",
    "  restore: bool\n",
    "    if True, restore the model from the most recent checkpoint and continue\n",
    "    training from there. If False, retrain the model from scratch.\n",
    "  \"\"\"\n",
    "  with self._graph._get_tf(\"Graph\").as_default():\n",
    "    step_count = [0]\n",
    "    workers = []\n",
    "    threads = []\n",
    "    for i in range(multiprocessing.cpu_count()):\n",
    "      workers.append(Worker(self, i))\n",
    "    self._session.run(tf.global_variables_initializer())\n",
    "    if restore:\n",
    "      self.restore()\n",
    "    for worker in workers:\n",
    "      thread = threading.Thread(\n",
    "          name=worker.scope,\n",
    "          target=lambda: worker.run(step_count, total_steps))\n",
    "      threads.append(thread)\n",
    "      thread.start()\n",
    "    variables = tf.get_collection(\n",
    "        tf.GraphKeys.GLOBAL_VARIABLES, scope=\"global\")\n",
    "    saver = tf.train.Saver(variables, max_to_keep=max_checkpoints_to_keep)\n",
    "    checkpoint_index = 0\n",
    "    while True:\n",
    "      threads = [t for t in threads if t.isAlive()]\n",
    "      if len(threads) > 0:\n",
    "        threads[0].join(checkpoint_interval)\n",
    "      checkpoint_index += 1\n",
    "      saver.save(\n",
    "          self._session, self._graph.save_file, global_step=checkpoint_index)\n",
    "      if len(threads) == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge for the Reader\n",
    "\n",
    "We strongly encourage you to try training tic-tac-toe models for yourself! Note that this example is more involved than other examples in the book, and will require greater computational power. We recommend a machine with at least a few CPU cores. This requirement isn’t too onerous; a good laptop should suffice. Try using a tool like htop to check that the code is indeed multithreaded. See how good a model you can train! You should be able to beat the random baseline most of the time, but this basic implementation won’t give you a model that always wins. We recommend exploring the RL literature and expanding upon the base implementation to see how well you can do."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
